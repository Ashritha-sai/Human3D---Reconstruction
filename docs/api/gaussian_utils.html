<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>gaussian_utils API documentation</title>
<meta name="description" content="Gaussian Splatting Utility Functions …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gaussian_utils</code></h1>
</header>
<section id="section-intro">
<p>Gaussian Splatting Utility Functions</p>
<p>This module provides mathematical utilities for 3D Gaussian Splatting,
including depth unprojection, scale estimation, color space conversion,
and rotation representations.</p>
<p>All functions are designed to work with both NumPy arrays and PyTorch tensors
where applicable, with explicit type hints for clarity.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gaussian_utils.build_covariance_3d"><code class="name flex">
<span>def <span class="ident">build_covariance_3d</span></span>(<span>scales: Union[np.ndarray, Tensor], rotations: Union[np.ndarray, Tensor]) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_covariance_3d(
    scales: Union[np.ndarray, Tensor],
    rotations: Union[np.ndarray, Tensor],
) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Build 3D covariance matrices from scales and rotations.

    Each 3D Gaussian is parameterized by axis-aligned scales and a rotation.
    The covariance matrix is: Sigma = R @ S @ S.T @ R.T
    where S = diag(scales) and R is the rotation matrix.

    Args:
        scales: Per-Gaussian axis scales.
            Shape: (N, 3)
            dtype: float32
            Units: Meters (standard deviation along each axis)

        rotations: Per-Gaussian rotation quaternions (w, x, y, z).
            Shape: (N, 4)
            dtype: float32
            Should be normalized

    Returns:
        covariances: 3x3 covariance matrices.
            Shape: (N, 3, 3)
            dtype: float32
            Symmetric positive semi-definite matrices

    Mathematical Operations:
        For each Gaussian i:
            S_i = diag(scales[i])                    # (3, 3) diagonal
            R_i = quaternion_to_rotation_matrix(rotations[i])  # (3, 3)
            Sigma_i = R_i @ S_i @ S_i.T @ R_i.T      # (3, 3)

    Example:
        &gt;&gt;&gt; scales = np.array([[0.1, 0.1, 0.1]])  # Isotropic
        &gt;&gt;&gt; rotations = np.array([[1, 0, 0, 0]])  # Identity
        &gt;&gt;&gt; cov = build_covariance_3d(scales, rotations)
        &gt;&gt;&gt; print(cov)  # diag([0.01, 0.01, 0.01])
    &#34;&#34;&#34;
    # TODO: Implement 3D covariance construction
    raise NotImplementedError(&#34;build_covariance_3d not yet implemented&#34;)</code></pre>
</details>
<div class="desc"><p>Build 3D covariance matrices from scales and rotations.</p>
<p>Each 3D Gaussian is parameterized by axis-aligned scales and a rotation.
The covariance matrix is: Sigma = R @ S @ S.T @ R.T
where S = diag(scales) and R is the rotation matrix.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scales</code></strong></dt>
<dd>Per-Gaussian axis scales.
Shape: (N, 3)
dtype: float32
Units: Meters (standard deviation along each axis)</dd>
<dt><strong><code>rotations</code></strong></dt>
<dd>Per-Gaussian rotation quaternions (w, x, y, z).
Shape: (N, 4)
dtype: float32
Should be normalized</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>covariances</code></dt>
<dd>3x3 covariance matrices.
Shape: (N, 3, 3)
dtype: float32
Symmetric positive semi-definite matrices</dd>
</dl>
<p>Mathematical Operations:
For each Gaussian i:
S_i = diag(scales[i])
# (3, 3) diagonal
R_i = quaternion_to_rotation_matrix(rotations[i])
# (3, 3)
Sigma_i = R_i @ S_i @ S_i.T @ R_i.T
# (3, 3)</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; scales = np.array([[0.1, 0.1, 0.1]])  # Isotropic
&gt;&gt;&gt; rotations = np.array([[1, 0, 0, 0]])  # Identity
&gt;&gt;&gt; cov = build_covariance_3d(scales, rotations)
&gt;&gt;&gt; print(cov)  # diag([0.01, 0.01, 0.01])
</code></pre></div>
</dd>
<dt id="gaussian_utils.create_camera_intrinsics"><code class="name flex">
<span>def <span class="ident">create_camera_intrinsics</span></span>(<span>fx: float, fy: float, cx: float, cy: float, device: str = 'cpu') ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_camera_intrinsics(
    fx: float,
    fy: float,
    cx: float,
    cy: float,
    device: str = &#34;cpu&#34;,
) -&gt; Tensor:
    &#34;&#34;&#34;
    Create a 3x3 camera intrinsics matrix K.

    Args:
        fx: Focal length in x (pixels).
        fy: Focal length in y (pixels).
        cx: Principal point x (pixels).
        cy: Principal point y (pixels).
        device: Torch device.

    Returns:
        K: Camera intrinsics matrix.
            Shape: (3, 3)
            Format: [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
    &#34;&#34;&#34;
    K = torch.tensor([
        [fx, 0, cx],
        [0, fy, cy],
        [0, 0, 1],
    ], dtype=torch.float32, device=device)
    return K</code></pre>
</details>
<div class="desc"><p>Create a 3x3 camera intrinsics matrix K.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>fx</code></strong></dt>
<dd>Focal length in x (pixels).</dd>
<dt><strong><code>fy</code></strong></dt>
<dd>Focal length in y (pixels).</dd>
<dt><strong><code>cx</code></strong></dt>
<dd>Principal point x (pixels).</dd>
<dt><strong><code>cy</code></strong></dt>
<dd>Principal point y (pixels).</dd>
<dt><strong><code>device</code></strong></dt>
<dd>Torch device.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>K</code></dt>
<dd>Camera intrinsics matrix.
Shape: (3, 3)
Format: [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]</dd>
</dl></div>
</dd>
<dt id="gaussian_utils.create_front_camera_pose"><code class="name flex">
<span>def <span class="ident">create_front_camera_pose</span></span>(<span>device: str = 'cpu') ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_front_camera_pose(
    device: str = &#34;cpu&#34;,
) -&gt; Tensor:
    &#34;&#34;&#34;
    Create identity camera pose (camera at origin looking down -Z).

    This is the default pose for rendering the front view, matching
    how the depth was captured (camera facing the subject).

    Args:
        device: Torch device.

    Returns:
        viewmat: 4x4 world-to-camera transformation matrix.
            Shape: (4, 4)
            Identity matrix (no transformation).

    Coordinate System:
        Camera coordinates:
            - X: Right
            - Y: Down
            - Z: Forward (into the scene)
    &#34;&#34;&#34;
    return torch.eye(4, dtype=torch.float32, device=device)</code></pre>
</details>
<div class="desc"><p>Create identity camera pose (camera at origin looking down -Z).</p>
<p>This is the default pose for rendering the front view, matching
how the depth was captured (camera facing the subject).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>device</code></strong></dt>
<dd>Torch device.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>viewmat</code></dt>
<dd>4x4 world-to-camera transformation matrix.
Shape: (4, 4)
Identity matrix (no transformation).</dd>
</dl>
<p>Coordinate System:
Camera coordinates:
- X: Right
- Y: Down
- Z: Forward (into the scene)</p></div>
</dd>
<dt id="gaussian_utils.create_orbit_camera_pose"><code class="name flex">
<span>def <span class="ident">create_orbit_camera_pose</span></span>(<span>azimuth: float,<br>elevation: float,<br>radius: float,<br>target: Tuple[float, float, float] = (0.0, 0.0, 1.5),<br>device: str = 'cpu') ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_orbit_camera_pose(
    azimuth: float,
    elevation: float,
    radius: float,
    target: Tuple[float, float, float] = (0.0, 0.0, 1.5),
    device: str = &#34;cpu&#34;,
) -&gt; Tensor:
    &#34;&#34;&#34;
    Create a camera pose orbiting around a target point.

    Useful for generating novel views by rotating the camera around
    the reconstructed subject.

    Args:
        azimuth: Horizontal rotation angle in radians.
            0 = front, pi/2 = left side, pi = back, -pi/2 = right side

        elevation: Vertical rotation angle in radians.
            0 = eye level, pi/4 = looking down from above, -pi/4 = looking up

        radius: Distance from target point in meters.

        target: 3D point to orbit around (x, y, z).
            Default: (0, 0, 1.5) - center of typical human subject

        device: Torch device.

    Returns:
        viewmat: 4x4 world-to-camera transformation matrix.
            Shape: (4, 4)

    Example:
        &gt;&gt;&gt; # Front view
        &gt;&gt;&gt; pose_front = create_orbit_camera_pose(0, 0, 2.0)

        &gt;&gt;&gt; # Left side view (90 degrees)
        &gt;&gt;&gt; pose_left = create_orbit_camera_pose(np.pi/2, 0, 2.0)

        &gt;&gt;&gt; # View from above
        &gt;&gt;&gt; pose_above = create_orbit_camera_pose(0, np.pi/4, 2.0)
    &#34;&#34;&#34;
    # Camera position on sphere
    x = radius * np.cos(elevation) * np.sin(azimuth)
    y = radius * np.sin(elevation)
    z = radius * np.cos(elevation) * np.cos(azimuth)

    camera_pos = np.array([x, y, z]) + np.array(target)

    # Look-at target
    forward = np.array(target) - camera_pos
    forward = forward / (np.linalg.norm(forward) + 1e-8)

    # Up vector (world Y is down, so use -Y as up)
    world_up = np.array([0.0, -1.0, 0.0])

    # Right vector
    right = np.cross(forward, world_up)
    right = right / (np.linalg.norm(right) + 1e-8)

    # Recompute up to ensure orthogonality
    up = np.cross(right, forward)
    up = up / (np.linalg.norm(up) + 1e-8)

    # Build rotation matrix (camera axes in world coordinates)
    # Note: camera looks along +Z, so forward = +Z
    R = np.stack([right, -up, forward], axis=1)  # (3, 3)

    # Build 4x4 transformation matrix (world to camera)
    # R^T @ (p - t) = camera coordinates
    viewmat = np.eye(4, dtype=np.float32)
    viewmat[:3, :3] = R.T
    viewmat[:3, 3] = -R.T @ camera_pos

    return torch.tensor(viewmat, dtype=torch.float32, device=device)</code></pre>
</details>
<div class="desc"><p>Create a camera pose orbiting around a target point.</p>
<p>Useful for generating novel views by rotating the camera around
the reconstructed subject.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>azimuth</code></strong></dt>
<dd>Horizontal rotation angle in radians.
0 = front, pi/2 = left side, pi = back, -pi/2 = right side</dd>
<dt><strong><code>elevation</code></strong></dt>
<dd>Vertical rotation angle in radians.
0 = eye level, pi/4 = looking down from above, -pi/4 = looking up</dd>
<dt><strong><code>radius</code></strong></dt>
<dd>Distance from target point in meters.</dd>
<dt><strong><code>target</code></strong></dt>
<dd>3D point to orbit around (x, y, z).
Default: (0, 0, 1.5) - center of typical human subject</dd>
<dt><strong><code>device</code></strong></dt>
<dd>Torch device.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>viewmat</code></dt>
<dd>4x4 world-to-camera transformation matrix.
Shape: (4, 4)</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; # Front view
&gt;&gt;&gt; pose_front = create_orbit_camera_pose(0, 0, 2.0)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; # Left side view (90 degrees)
&gt;&gt;&gt; pose_left = create_orbit_camera_pose(np.pi/2, 0, 2.0)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; # View from above
&gt;&gt;&gt; pose_above = create_orbit_camera_pose(0, np.pi/4, 2.0)
</code></pre></div>
</dd>
<dt id="gaussian_utils.depth_to_xyz"><code class="name flex">
<span>def <span class="ident">depth_to_xyz</span></span>(<span>depth: Union[np.ndarray, Tensor],<br>mask: Union[np.ndarray, Tensor],<br>fx: float,<br>fy: float,<br>cx: float,<br>cy: float) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def depth_to_xyz(
    depth: Union[np.ndarray, Tensor],
    mask: Union[np.ndarray, Tensor],
    fx: float,
    fy: float,
    cx: float,
    cy: float,
) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Unproject depth map to 3D point cloud using pinhole camera model.

    Converts 2D pixel coordinates with depth values to 3D world coordinates
    using the standard pinhole camera unprojection equations.

    Args:
        depth: Depth map with metric or relative depth values.
            Shape: (H, W)
            dtype: float32
            Range: Positive values, where higher = farther from camera
            Units: Meters (for metric depth) or arbitrary (for relative)

        mask: Binary mask indicating which pixels to unproject.
            Shape: (H, W)
            dtype: bool, uint8, or float
            Values: Non-zero values indicate valid pixels

        fx: Focal length in x direction.
            Units: pixels
            Typical range: 500-2000 for standard cameras

        fy: Focal length in y direction.
            Units: pixels
            Typical range: 500-2000 for standard cameras

        cx: Principal point x coordinate (optical center).
            Units: pixels
            Typical value: width / 2

        cy: Principal point y coordinate (optical center).
            Units: pixels
            Typical value: height / 2

    Returns:
        xyz: 3D coordinates of valid (masked) pixels.
            Shape: (N, 3) where N = number of non-zero mask pixels
            dtype: Same as input depth (float32)
            Columns: [X, Y, Z] in camera coordinate system
                - X: Right (positive) / Left (negative)
                - Y: Down (positive) / Up (negative)
                - Z: Forward (positive, same as depth)

    Mathematical Operations:
        For each pixel (u, v) with depth d where mask[v, u] != 0:
            X = (u - cx) * d / fx
            Y = (v - cy) * d / fy
            Z = d

        This is the inverse of the projection:
            u = fx * X / Z + cx
            v = fy * Y / Z + cy

    Coordinate System:
        Camera coordinates (right-handed):
            - Origin: Camera optical center
            - X-axis: Right
            - Y-axis: Down
            - Z-axis: Forward (into the scene)

    Example:
        &gt;&gt;&gt; depth = np.random.rand(480, 640).astype(np.float32) * 5  # 0-5 meters
        &gt;&gt;&gt; mask = np.ones((480, 640), dtype=np.uint8)
        &gt;&gt;&gt; xyz = depth_to_xyz(depth, mask, fx=525, fy=525, cx=320, cy=240)
        &gt;&gt;&gt; print(xyz.shape)  # (307200, 3)

        &gt;&gt;&gt; # With sparse mask
        &gt;&gt;&gt; mask = np.zeros((480, 640), dtype=np.uint8)
        &gt;&gt;&gt; mask[100:200, 150:250] = 1  # Only unproject a region
        &gt;&gt;&gt; xyz = depth_to_xyz(depth, mask, fx=525, fy=525, cx=320, cy=240)
        &gt;&gt;&gt; print(xyz.shape)  # (10000, 3) = 100 * 100 pixels
    &#34;&#34;&#34;
    is_tensor = isinstance(depth, Tensor)

    if is_tensor:
        # PyTorch implementation
        h, w = depth.shape[:2]
        device = depth.device

        # Create meshgrid of pixel coordinates
        ys, xs = torch.meshgrid(
            torch.arange(h, device=device, dtype=torch.float32),
            torch.arange(w, device=device, dtype=torch.float32),
            indexing=&#39;ij&#39;
        )

        # Get the Z values (depth)
        z = depth.clone()

        # Compute X and Y using pinhole model
        X = (xs - cx) * z / fx
        Y = (ys - cy) * z / fy
        Z = z

        # Stack to (H, W, 3)
        xyz_full = torch.stack([X, Y, Z], dim=-1)

        # Apply mask
        mask_bool = mask.bool() if not mask.dtype == torch.bool else mask

        # Also filter out invalid depth (zero or negative)
        valid = mask_bool &amp; (z &gt; 0) &amp; torch.isfinite(z)

        # Flatten and filter
        xyz_flat = xyz_full.reshape(-1, 3)
        valid_flat = valid.reshape(-1)

        xyz = xyz_flat[valid_flat]

        return xyz.float()

    else:
        # NumPy implementation
        h, w = depth.shape[:2]

        # Create meshgrid of pixel coordinates
        xs, ys = np.meshgrid(np.arange(w), np.arange(h))

        # Get the Z values (depth)
        z = depth.copy()

        # Compute X and Y using pinhole model
        X = (xs - cx) * z / fx
        Y = (ys - cy) * z / fy
        Z = z

        # Stack to (H, W, 3)
        xyz_full = np.stack([X, Y, Z], axis=-1)

        # Apply mask - convert to boolean
        mask_bool = mask.astype(bool) if mask.dtype != bool else mask

        # Also filter out invalid depth (zero or negative)
        valid = mask_bool &amp; (z &gt; 0) &amp; np.isfinite(z)

        # Flatten and filter
        xyz_flat = xyz_full.reshape(-1, 3)
        valid_flat = valid.reshape(-1)

        xyz = xyz_flat[valid_flat]

        return xyz.astype(np.float32)</code></pre>
</details>
<div class="desc"><p>Unproject depth map to 3D point cloud using pinhole camera model.</p>
<p>Converts 2D pixel coordinates with depth values to 3D world coordinates
using the standard pinhole camera unprojection equations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>depth</code></strong></dt>
<dd>Depth map with metric or relative depth values.
Shape: (H, W)
dtype: float32
Range: Positive values, where higher = farther from camera
Units: Meters (for metric depth) or arbitrary (for relative)</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Binary mask indicating which pixels to unproject.
Shape: (H, W)
dtype: bool, uint8, or float
Values: Non-zero values indicate valid pixels</dd>
<dt><strong><code>fx</code></strong></dt>
<dd>Focal length in x direction.
Units: pixels
Typical range: 500-2000 for standard cameras</dd>
<dt><strong><code>fy</code></strong></dt>
<dd>Focal length in y direction.
Units: pixels
Typical range: 500-2000 for standard cameras</dd>
<dt><strong><code>cx</code></strong></dt>
<dd>Principal point x coordinate (optical center).
Units: pixels
Typical value: width / 2</dd>
<dt><strong><code>cy</code></strong></dt>
<dd>Principal point y coordinate (optical center).
Units: pixels
Typical value: height / 2</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xyz</code></dt>
<dd>3D coordinates of valid (masked) pixels.
Shape: (N, 3) where N = number of non-zero mask pixels
dtype: Same as input depth (float32)
Columns: [X, Y, Z] in camera coordinate system
- X: Right (positive) / Left (negative)
- Y: Down (positive) / Up (negative)
- Z: Forward (positive, same as depth)</dd>
</dl>
<p>Mathematical Operations:
For each pixel (u, v) with depth d where mask[v, u] != 0:
X = (u - cx) * d / fx
Y = (v - cy) * d / fy
Z = d</p>
<pre><code>This is the inverse of the projection:
    u = fx * X / Z + cx
    v = fy * Y / Z + cy
</code></pre>
<p>Coordinate System:
Camera coordinates (right-handed):
- Origin: Camera optical center
- X-axis: Right
- Y-axis: Down
- Z-axis: Forward (into the scene)</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; depth = np.random.rand(480, 640).astype(np.float32) * 5  # 0-5 meters
&gt;&gt;&gt; mask = np.ones((480, 640), dtype=np.uint8)
&gt;&gt;&gt; xyz = depth_to_xyz(depth, mask, fx=525, fy=525, cx=320, cy=240)
&gt;&gt;&gt; print(xyz.shape)  # (307200, 3)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; # With sparse mask
&gt;&gt;&gt; mask = np.zeros((480, 640), dtype=np.uint8)
&gt;&gt;&gt; mask[100:200, 150:250] = 1  # Only unproject a region
&gt;&gt;&gt; xyz = depth_to_xyz(depth, mask, fx=525, fy=525, cx=320, cy=240)
&gt;&gt;&gt; print(xyz.shape)  # (10000, 3) = 100 * 100 pixels
</code></pre></div>
</dd>
<dt id="gaussian_utils.estimate_point_scales"><code class="name flex">
<span>def <span class="ident">estimate_point_scales</span></span>(<span>xyz_points: Union[np.ndarray, Tensor], k_neighbors: int = 8) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimate_point_scales(
    xyz_points: Union[np.ndarray, Tensor],
    k_neighbors: int = 8,
) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Estimate initial Gaussian scales based on local point density.

    Computes an appropriate scale for each Gaussian by finding the average
    distance to k nearest neighbors. This ensures Gaussians are sized
    appropriately for the local point cloud density.

    Args:
        xyz_points: 3D point positions.
            Shape: (N, 3)
            dtype: float32
            Units: Meters (same as depth input)

        k_neighbors: Number of nearest neighbors to consider.
            Default: 8
            Range: 1-20 typically
            Higher values = smoother scale estimates

    Returns:
        scales: Estimated scale for each point.
            Shape: (N, 3) - isotropic scale repeated for x, y, z
            dtype: float32
            Units: Meters
            Range: Positive values

    Mathematical Operations:
        For each point p_i:
            1. Find k nearest neighbors: {p_j1, p_j2, ..., p_jk}
            2. Compute distances: d_j = ||p_i - p_j||
            3. Scale estimate: s_i = mean(d_j) / 2

        The division by 2 ensures neighboring Gaussians overlap
        appropriately (each Gaussian extends ~2 sigma).

    Performance Notes:
        - Uses scipy.spatial.KDTree for CPU arrays
        - Uses torch-cluster or manual computation for GPU tensors
        - Complexity: O(N log N) for KDTree construction + O(N * k) for queries

    Example:
        &gt;&gt;&gt; xyz = np.random.rand(1000, 3).astype(np.float32)
        &gt;&gt;&gt; scales = estimate_point_scales(xyz, k_neighbors=8)
        &gt;&gt;&gt; print(scales.shape)  # (1000, 3)
        &gt;&gt;&gt; print(scales.mean())  # ~0.05 for uniform [0,1] distribution
    &#34;&#34;&#34;
    is_tensor = isinstance(xyz_points, Tensor)

    if is_tensor:
        device = xyz_points.device
        n_points = xyz_points.shape[0]

        # For CPU tensors or when pytorch3d not available, use scipy via numpy
        use_scipy = device.type == &#39;cpu&#39;

        if not use_scipy:
            try:
                from pytorch3d.ops import knn_points
            except ImportError:
                use_scipy = True

        if use_scipy:
            # Convert to numpy, use scipy, convert back
            from scipy.spatial import KDTree

            xyz_np = xyz_points.detach().cpu().numpy()

            if n_points == 0:
                return torch.zeros((0, 3), dtype=torch.float32, device=device)

            tree = KDTree(xyz_np)
            k_query = min(k_neighbors + 1, n_points)
            distances, _ = tree.query(xyz_np, k=k_query)

            if k_query &gt; 1:
                distances_neighbors = distances[:, 1:]
            else:
                distances_neighbors = distances

            mean_dist = np.mean(distances_neighbors, axis=1)
            scale = mean_dist / 2.0
            scale = np.clip(scale, 1e-6, 10.0)
            scales = np.stack([scale, scale, scale], axis=1)

            return torch.from_numpy(scales.astype(np.float32)).to(device)

        else:
            # PyTorch implementation using pytorch3d.ops.knn_points
            from pytorch3d.ops import knn_points

            # knn_points expects (B, N, 3) format
            points = xyz_points.unsqueeze(0)  # (1, N, 3)

            # Find k+1 nearest neighbors (includes self at index 0)
            k_query = min(k_neighbors + 1, n_points)
            knn_result = knn_points(points, points, K=k_query, return_nn=False)

            # knn_result.dists is squared distances, shape (1, N, K)
            # First column is distance to self (should be 0)
            sq_dists = knn_result.dists[0]  # (N, K)

            # Take distances to neighbors (exclude self at index 0)
            if k_query &gt; 1:
                sq_dists_neighbors = sq_dists[:, 1:]  # (N, K-1)
            else:
                # Edge case: only 1 point
                sq_dists_neighbors = sq_dists

            # Compute mean distance (take sqrt of squared distances)
            dists = torch.sqrt(sq_dists_neighbors + 1e-10)
            mean_dist = dists.mean(dim=1)  # (N,)

            # Scale estimate = mean_dist / 2
            scale = mean_dist / 2.0

            # Clamp to reasonable range
            scale = torch.clamp(scale, min=1e-6, max=10.0)

            # Repeat for isotropic (N, 3)
            scales = scale.unsqueeze(1).repeat(1, 3)

            return scales.float()

    else:
        # NumPy implementation using scipy KDTree
        from scipy.spatial import KDTree

        n_points = xyz_points.shape[0]

        if n_points == 0:
            return np.zeros((0, 3), dtype=np.float32)

        # Build KDTree
        tree = KDTree(xyz_points)

        # Query k+1 nearest neighbors (includes self)
        k_query = min(k_neighbors + 1, n_points)
        distances, _ = tree.query(xyz_points, k=k_query)

        # distances shape: (N, k_query)
        # First column is distance to self (0), exclude it
        if k_query &gt; 1:
            distances_neighbors = distances[:, 1:]
        else:
            distances_neighbors = distances

        # Compute mean distance
        mean_dist = np.mean(distances_neighbors, axis=1)

        # Scale estimate = mean_dist / 2
        scale = mean_dist / 2.0

        # Clamp to reasonable range
        scale = np.clip(scale, 1e-6, 10.0)

        # Repeat for isotropic (N, 3)
        scales = np.stack([scale, scale, scale], axis=1)

        return scales.astype(np.float32)</code></pre>
</details>
<div class="desc"><p>Estimate initial Gaussian scales based on local point density.</p>
<p>Computes an appropriate scale for each Gaussian by finding the average
distance to k nearest neighbors. This ensures Gaussians are sized
appropriately for the local point cloud density.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>xyz_points</code></strong></dt>
<dd>3D point positions.
Shape: (N, 3)
dtype: float32
Units: Meters (same as depth input)</dd>
<dt><strong><code>k_neighbors</code></strong></dt>
<dd>Number of nearest neighbors to consider.
Default: 8
Range: 1-20 typically
Higher values = smoother scale estimates</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>scales</code></dt>
<dd>Estimated scale for each point.
Shape: (N, 3) - isotropic scale repeated for x, y, z
dtype: float32
Units: Meters
Range: Positive values</dd>
</dl>
<p>Mathematical Operations:
For each point p_i:
1. Find k nearest neighbors: {p_j1, p_j2, &hellip;, p_jk}
2. Compute distances: d_j = ||p_i - p_j||
3. Scale estimate: s_i = mean(d_j) / 2</p>
<pre><code>The division by 2 ensures neighboring Gaussians overlap
appropriately (each Gaussian extends ~2 sigma).
</code></pre>
<p>Performance Notes:
- Uses scipy.spatial.KDTree for CPU arrays
- Uses torch-cluster or manual computation for GPU tensors
- Complexity: O(N log N) for KDTree construction + O(N * k) for queries</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; xyz = np.random.rand(1000, 3).astype(np.float32)
&gt;&gt;&gt; scales = estimate_point_scales(xyz, k_neighbors=8)
&gt;&gt;&gt; print(scales.shape)  # (1000, 3)
&gt;&gt;&gt; print(scales.mean())  # ~0.05 for uniform [0,1] distribution
</code></pre></div>
</dd>
<dt id="gaussian_utils.get_view_direction"><code class="name flex">
<span>def <span class="ident">get_view_direction</span></span>(<span>means: Tensor, camera_pos: Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_view_direction(
    means: Tensor,
    camera_pos: Tensor,
) -&gt; Tensor:
    &#34;&#34;&#34;
    Compute view directions from Gaussian centers to camera.

    Used for evaluating spherical harmonics at the correct view angle.

    Args:
        means: Gaussian center positions.
            Shape: (N, 3)

        camera_pos: Camera position in world coordinates.
            Shape: (3,)

    Returns:
        view_dirs: Normalized view direction vectors.
            Shape: (N, 3)
            Points from each Gaussian toward the camera.
    &#34;&#34;&#34;
    # Direction from each Gaussian to camera
    dirs = camera_pos.unsqueeze(0) - means  # (N, 3)

    # Normalize
    dirs = dirs / (torch.norm(dirs, dim=-1, keepdim=True) + 1e-8)

    return dirs</code></pre>
</details>
<div class="desc"><p>Compute view directions from Gaussian centers to camera.</p>
<p>Used for evaluating spherical harmonics at the correct view angle.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>means</code></strong></dt>
<dd>Gaussian center positions.
Shape: (N, 3)</dd>
<dt><strong><code>camera_pos</code></strong></dt>
<dd>Camera position in world coordinates.
Shape: (3,)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>view_dirs</code></dt>
<dd>Normalized view direction vectors.
Shape: (N, 3)
Points from each Gaussian toward the camera.</dd>
</dl></div>
</dd>
<dt id="gaussian_utils.inverse_sigmoid"><code class="name flex">
<span>def <span class="ident">inverse_sigmoid</span></span>(<span>x: Union[np.ndarray, Tensor]) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_sigmoid(x: Union[np.ndarray, Tensor]) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Inverse sigmoid (logit) function.

    Used to convert [0, 1] opacity to logit space for optimization.

    Args:
        x: Input values.
            Shape: Any
            dtype: float32
            Range: (0, 1) exclusive

    Returns:
        Output values in logit space.
            Shape: Same as input
            Range: (-inf, inf)
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        return torch.log(x / (1.0 - x))
    return np.log(x / (1.0 - x))</code></pre>
</details>
<div class="desc"><p>Inverse sigmoid (logit) function.</p>
<p>Used to convert [0, 1] opacity to logit space for optimization.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input values.
Shape: Any
dtype: float32
Range: (0, 1) exclusive</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>Output values in logit space.</dt>
<dt><code>
Shape</code></dt>
<dd>Same as input
Range: (-inf, inf)</dd>
</dl></div>
</dd>
<dt id="gaussian_utils.normalize_depth_to_metric"><code class="name flex">
<span>def <span class="ident">normalize_depth_to_metric</span></span>(<span>depth: Union[np.ndarray, Tensor],<br>min_depth: float = 0.5,<br>max_depth: float = 2.5) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_depth_to_metric(
    depth: Union[np.ndarray, Tensor],
    min_depth: float = 0.5,
    max_depth: float = 2.5,
) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Normalize relative depth to pseudo-metric range.

    This matches the normalization used in the original pointcloud.py:
    the depth is normalized to [0, 1] and then scaled to [min_depth, max_depth].

    Args:
        depth: Raw depth map (relative or inverse depth from MiDaS).
            Shape: (H, W)
            dtype: float32

        min_depth: Minimum depth value in output range (meters).
            Default: 0.5

        max_depth: Maximum depth value in output range (meters).
            Default: 2.5

    Returns:
        Normalized depth map in pseudo-metric range.
            Shape: (H, W)
            Range: [min_depth, max_depth]
    &#34;&#34;&#34;
    is_tensor = isinstance(depth, Tensor)

    if is_tensor:
        d = depth.clone()
        d = d - torch.min(d[torch.isfinite(d)])
        denom = torch.max(d[torch.isfinite(d)]) + 1e-6
        d = d / denom
        z = min_depth + (max_depth - min_depth) * d
        return z
    else:
        d = depth.copy()
        d = d - np.nanmin(d)
        denom = np.nanmax(d) + 1e-6
        d = d / denom
        z = min_depth + (max_depth - min_depth) * d
        return z.astype(np.float32)</code></pre>
</details>
<div class="desc"><p>Normalize relative depth to pseudo-metric range.</p>
<p>This matches the normalization used in the original pointcloud.py:
the depth is normalized to [0, 1] and then scaled to [min_depth, max_depth].</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>depth</code></strong></dt>
<dd>Raw depth map (relative or inverse depth from MiDaS).
Shape: (H, W)
dtype: float32</dd>
<dt><strong><code>min_depth</code></strong></dt>
<dd>Minimum depth value in output range (meters).
Default: 0.5</dd>
<dt><strong><code>max_depth</code></strong></dt>
<dd>Maximum depth value in output range (meters).
Default: 2.5</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>Normalized depth map in pseudo-metric range.</dt>
<dt><code>
Shape</code></dt>
<dd>(H, W)
Range: [min_depth, max_depth]</dd>
</dl></div>
</dd>
<dt id="gaussian_utils.project_covariance_to_2d"><code class="name flex">
<span>def <span class="ident">project_covariance_to_2d</span></span>(<span>cov_3d: Union[np.ndarray, Tensor],<br>means_cam: Union[np.ndarray, Tensor],<br>fx: float,<br>fy: float) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def project_covariance_to_2d(
    cov_3d: Union[np.ndarray, Tensor],
    means_cam: Union[np.ndarray, Tensor],
    fx: float,
    fy: float,
) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Project 3D covariance to 2D image plane covariance.

    Uses the Jacobian of the projection to transform 3D Gaussian
    covariances to their 2D equivalents for rasterization.

    Args:
        cov_3d: 3D covariance matrices.
            Shape: (N, 3, 3)
            dtype: float32

        means_cam: Gaussian centers in camera coordinates.
            Shape: (N, 3)
            dtype: float32
            [X, Y, Z] where Z &gt; 0 (in front of camera)

        fx: Focal length x (pixels).
        fy: Focal length y (pixels).

    Returns:
        cov_2d: 2D covariance matrices.
            Shape: (N, 2, 2)
            dtype: float32

    Mathematical Operations:
        Jacobian of perspective projection at point (X, Y, Z):
            J = [fx/Z,  0,    -fx*X/Z²]
                [0,     fy/Z, -fy*Y/Z²]

        2D covariance: Sigma_2d = J @ Sigma_3d @ J.T

    Example:
        &gt;&gt;&gt; cov_3d = np.eye(3).reshape(1, 3, 3) * 0.01
        &gt;&gt;&gt; means = np.array([[0, 0, 2.0]])  # 2 meters in front
        &gt;&gt;&gt; cov_2d = project_covariance_to_2d(cov_3d, means, fx=500, fy=500)
    &#34;&#34;&#34;
    # TODO: Implement 2D covariance projection
    raise NotImplementedError(&#34;project_covariance_to_2d not yet implemented&#34;)</code></pre>
</details>
<div class="desc"><p>Project 3D covariance to 2D image plane covariance.</p>
<p>Uses the Jacobian of the projection to transform 3D Gaussian
covariances to their 2D equivalents for rasterization.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cov_3d</code></strong></dt>
<dd>3D covariance matrices.
Shape: (N, 3, 3)
dtype: float32</dd>
<dt><strong><code>means_cam</code></strong></dt>
<dd>Gaussian centers in camera coordinates.
Shape: (N, 3)
dtype: float32
[X, Y, Z] where Z &gt; 0 (in front of camera)</dd>
<dt><strong><code>fx</code></strong></dt>
<dd>Focal length x (pixels).</dd>
<dt><strong><code>fy</code></strong></dt>
<dd>Focal length y (pixels).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>cov_2d</code></dt>
<dd>2D covariance matrices.
Shape: (N, 2, 2)
dtype: float32</dd>
</dl>
<p>Mathematical Operations:
Jacobian of perspective projection at point (X, Y, Z):
J = [fx/Z,
0,
-fx<em>X/Z²]
[0,
fy/Z, -fy</em>Y/Z²]</p>
<pre><code>2D covariance: Sigma_2d = J @ Sigma_3d @ J.T
</code></pre>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; cov_3d = np.eye(3).reshape(1, 3, 3) * 0.01
&gt;&gt;&gt; means = np.array([[0, 0, 2.0]])  # 2 meters in front
&gt;&gt;&gt; cov_2d = project_covariance_to_2d(cov_3d, means, fx=500, fy=500)
</code></pre></div>
</dd>
<dt id="gaussian_utils.quaternion_to_rotation_matrix"><code class="name flex">
<span>def <span class="ident">quaternion_to_rotation_matrix</span></span>(<span>quats: Union[np.ndarray, Tensor]) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quaternion_to_rotation_matrix(
    quats: Union[np.ndarray, Tensor],
) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Convert quaternions to 3x3 rotation matrices.

    Uses the Hamilton convention with quaternion format (w, x, y, z)
    where w is the scalar component.

    Args:
        quats: Quaternions in (w, x, y, z) format.
            Shape: (N, 4) or (4,)
            dtype: float32
            Should be normalized (||q|| = 1), but will work with unnormalized

    Returns:
        rot_matrices: 3x3 rotation matrices.
            Shape: (N, 3, 3) or (3, 3)
            dtype: float32
            Orthogonal matrices with det = +1

    Mathematical Operations:
        For quaternion q = (w, x, y, z):

        R = [1 - 2(y² + z²),    2(xy - wz),      2(xz + wy)    ]
            [2(xy + wz),        1 - 2(x² + z²),  2(yz - wx)    ]
            [2(xz - wy),        2(yz + wx),      1 - 2(x² + y²)]

    Conventions:
        - Hamilton convention: q = w + xi + yj + zk
        - Right-handed coordinate system
        - Active rotation (rotates vectors, not coordinate frame)

    Example:
        &gt;&gt;&gt; # Identity rotation
        &gt;&gt;&gt; q = np.array([[1.0, 0.0, 0.0, 0.0]])
        &gt;&gt;&gt; R = quaternion_to_rotation_matrix(q)
        &gt;&gt;&gt; print(R)  # 3x3 identity matrix

        &gt;&gt;&gt; # 90-degree rotation around Z-axis
        &gt;&gt;&gt; q = np.array([[np.cos(np.pi/4), 0, 0, np.sin(np.pi/4)]])
        &gt;&gt;&gt; R = quaternion_to_rotation_matrix(q)
    &#34;&#34;&#34;
    is_tensor = isinstance(quats, Tensor)
    single = quats.ndim == 1

    if single:
        quats = quats[None, :]  # Add batch dimension

    if is_tensor:
        # Normalize quaternions
        quats = quats / (torch.norm(quats, dim=-1, keepdim=True) + 1e-8)

        w, x, y, z = quats[:, 0], quats[:, 1], quats[:, 2], quats[:, 3]

        # Compute rotation matrix elements
        r00 = 1 - 2 * (y * y + z * z)
        r01 = 2 * (x * y - w * z)
        r02 = 2 * (x * z + w * y)

        r10 = 2 * (x * y + w * z)
        r11 = 1 - 2 * (x * x + z * z)
        r12 = 2 * (y * z - w * x)

        r20 = 2 * (x * z - w * y)
        r21 = 2 * (y * z + w * x)
        r22 = 1 - 2 * (x * x + y * y)

        # Stack into (N, 3, 3)
        R = torch.stack([
            torch.stack([r00, r01, r02], dim=-1),
            torch.stack([r10, r11, r12], dim=-1),
            torch.stack([r20, r21, r22], dim=-1),
        ], dim=1)

        if single:
            R = R[0]

        return R.float()

    else:
        # NumPy implementation
        quats = quats / (np.linalg.norm(quats, axis=-1, keepdims=True) + 1e-8)

        w, x, y, z = quats[:, 0], quats[:, 1], quats[:, 2], quats[:, 3]

        r00 = 1 - 2 * (y * y + z * z)
        r01 = 2 * (x * y - w * z)
        r02 = 2 * (x * z + w * y)

        r10 = 2 * (x * y + w * z)
        r11 = 1 - 2 * (x * x + z * z)
        r12 = 2 * (y * z - w * x)

        r20 = 2 * (x * z - w * y)
        r21 = 2 * (y * z + w * x)
        r22 = 1 - 2 * (x * x + y * y)

        R = np.stack([
            np.stack([r00, r01, r02], axis=-1),
            np.stack([r10, r11, r12], axis=-1),
            np.stack([r20, r21, r22], axis=-1),
        ], axis=1)

        if single:
            R = R[0]

        return R.astype(np.float32)</code></pre>
</details>
<div class="desc"><p>Convert quaternions to 3x3 rotation matrices.</p>
<p>Uses the Hamilton convention with quaternion format (w, x, y, z)
where w is the scalar component.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>quats</code></strong></dt>
<dd>Quaternions in (w, x, y, z) format.
Shape: (N, 4) or (4,)
dtype: float32
Should be normalized (||q|| = 1), but will work with unnormalized</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>rot_matrices</code></dt>
<dd>3x3 rotation matrices.
Shape: (N, 3, 3) or (3, 3)
dtype: float32
Orthogonal matrices with det = +1</dd>
</dl>
<p>Mathematical Operations:
For quaternion q = (w, x, y, z):</p>
<pre><code>R = [1 - 2(y² + z²),    2(xy - wz),      2(xz + wy)    ]
    [2(xy + wz),        1 - 2(x² + z²),  2(yz - wx)    ]
    [2(xz - wy),        2(yz + wx),      1 - 2(x² + y²)]
</code></pre>
<h2 id="conventions">Conventions</h2>
<ul>
<li>Hamilton convention: q = w + xi + yj + zk</li>
<li>Right-handed coordinate system</li>
<li>Active rotation (rotates vectors, not coordinate frame)</li>
</ul>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; # Identity rotation
&gt;&gt;&gt; q = np.array([[1.0, 0.0, 0.0, 0.0]])
&gt;&gt;&gt; R = quaternion_to_rotation_matrix(q)
&gt;&gt;&gt; print(R)  # 3x3 identity matrix
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; # 90-degree rotation around Z-axis
&gt;&gt;&gt; q = np.array([[np.cos(np.pi/4), 0, 0, np.sin(np.pi/4)]])
&gt;&gt;&gt; R = quaternion_to_rotation_matrix(q)
</code></pre></div>
</dd>
<dt id="gaussian_utils.rgb_to_spherical_harmonics"><code class="name flex">
<span>def <span class="ident">rgb_to_spherical_harmonics</span></span>(<span>rgb_colors: Union[np.ndarray, Tensor], degree: int = 0) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rgb_to_spherical_harmonics(
    rgb_colors: Union[np.ndarray, Tensor],
    degree: int = 0,
) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Convert RGB colors to spherical harmonic coefficients.

    For view-independent color (degree=0), this simply converts RGB to
    the DC (constant) term of spherical harmonics. Higher degrees enable
    view-dependent color effects.

    Args:
        rgb_colors: RGB color values.
            Shape: (N, 3)
            dtype: float32
            Range: [0, 1] normalized

        degree: Maximum spherical harmonic degree.
            0: DC only (1 coefficient per channel) - view-independent
            1: DC + 3 first-order (4 coefficients per channel)
            2: DC + first + second order (9 coefficients per channel)
            3: Full (16 coefficients per channel)
            Default: 0

    Returns:
        sh_coeffs: Spherical harmonic coefficients.
            Shape: (N, num_coeffs, 3) where num_coeffs = (degree + 1)^2
            dtype: float32
            Range: Unbounded (not normalized)

    Mathematical Operations:
        DC term (l=0, m=0):
            The zeroth-order SH is constant: Y_0^0 = 1 / (2 * sqrt(pi))
            To encode color c, we set: sh_dc = c / Y_0^0 = c * 2 * sqrt(pi)

        Higher orders encode view-dependent variations.

    Spherical Harmonic Basis:
        Degree 0: 1 function (constant)
        Degree 1: 3 functions (linear in x, y, z)
        Degree 2: 5 functions (quadratic)
        Degree 3: 7 functions (cubic)
        Total for degree d: (d+1)^2 functions

    Example:
        &gt;&gt;&gt; rgb = np.array([[1.0, 0.0, 0.0],   # Red
        ...                  [0.0, 1.0, 0.0],   # Green
        ...                  [0.0, 0.0, 1.0]])  # Blue
        &gt;&gt;&gt; sh = rgb_to_spherical_harmonics(rgb, degree=0)
        &gt;&gt;&gt; print(sh.shape)  # (3, 1, 3)
    &#34;&#34;&#34;
    # SH constant for DC term: C0 = 0.5 / sqrt(pi) = 0.28209479177387814
    # To store color c such that evaluating SH gives back c:
    # sh_dc = (c - 0.5) / C0  (centered around 0.5 for 3DGS convention)
    C0 = 0.28209479177387814

    is_tensor = isinstance(rgb_colors, Tensor)
    n_points = rgb_colors.shape[0]
    num_coeffs = (degree + 1) ** 2

    if is_tensor:
        device = rgb_colors.device

        # Initialize output with zeros: (N, num_coeffs, 3)
        sh_coeffs = torch.zeros((n_points, num_coeffs, 3), dtype=torch.float32, device=device)

        # Set DC term: convert RGB [0,1] to SH space
        # Following 3DGS convention: sh_dc = (rgb - 0.5) / C0
        sh_coeffs[:, 0, :] = (rgb_colors - 0.5) / C0

        return sh_coeffs

    else:
        # NumPy implementation
        sh_coeffs = np.zeros((n_points, num_coeffs, 3), dtype=np.float32)

        # Set DC term
        sh_coeffs[:, 0, :] = (rgb_colors - 0.5) / C0

        return sh_coeffs</code></pre>
</details>
<div class="desc"><p>Convert RGB colors to spherical harmonic coefficients.</p>
<p>For view-independent color (degree=0), this simply converts RGB to
the DC (constant) term of spherical harmonics. Higher degrees enable
view-dependent color effects.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rgb_colors</code></strong></dt>
<dd>RGB color values.
Shape: (N, 3)
dtype: float32
Range: [0, 1] normalized</dd>
<dt><strong><code>degree</code></strong></dt>
<dd>Maximum spherical harmonic degree.
0: DC only (1 coefficient per channel) - view-independent
1: DC + 3 first-order (4 coefficients per channel)
2: DC + first + second order (9 coefficients per channel)
3: Full (16 coefficients per channel)
Default: 0</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>sh_coeffs</code></dt>
<dd>Spherical harmonic coefficients.
Shape: (N, num_coeffs, 3) where num_coeffs = (degree + 1)^2
dtype: float32
Range: Unbounded (not normalized)</dd>
</dl>
<p>Mathematical Operations:
DC term (l=0, m=0):
The zeroth-order SH is constant: Y_0^0 = 1 / (2 * sqrt(pi))
To encode color c, we set: sh_dc = c / Y_0^0 = c * 2 * sqrt(pi)</p>
<pre><code>Higher orders encode view-dependent variations.
</code></pre>
<p>Spherical Harmonic Basis:
Degree 0: 1 function (constant)
Degree 1: 3 functions (linear in x, y, z)
Degree 2: 5 functions (quadratic)
Degree 3: 7 functions (cubic)
Total for degree d: (d+1)^2 functions</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; rgb = np.array([[1.0, 0.0, 0.0],   # Red
...                  [0.0, 1.0, 0.0],   # Green
...                  [0.0, 0.0, 1.0]])  # Blue
&gt;&gt;&gt; sh = rgb_to_spherical_harmonics(rgb, degree=0)
&gt;&gt;&gt; print(sh.shape)  # (3, 1, 3)
</code></pre></div>
</dd>
<dt id="gaussian_utils.rotation_matrix_to_quaternion"><code class="name flex">
<span>def <span class="ident">rotation_matrix_to_quaternion</span></span>(<span>rot_matrices: Union[np.ndarray, Tensor]) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rotation_matrix_to_quaternion(
    rot_matrices: Union[np.ndarray, Tensor],
) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Convert 3x3 rotation matrices to quaternions.

    Inverse of quaternion_to_rotation_matrix. Uses Shepperd&#39;s method
    for numerical stability.

    Args:
        rot_matrices: 3x3 rotation matrices.
            Shape: (N, 3, 3) or (3, 3)
            dtype: float32
            Should be orthogonal with det = +1

    Returns:
        quats: Quaternions in (w, x, y, z) format.
            Shape: (N, 4) or (4,)
            dtype: float32
            Normalized (||q|| = 1)

    Example:
        &gt;&gt;&gt; R = np.eye(3, dtype=np.float32)
        &gt;&gt;&gt; q = rotation_matrix_to_quaternion(R)
        &gt;&gt;&gt; print(q)  # [1, 0, 0, 0]
    &#34;&#34;&#34;
    # TODO: Implement rotation matrix to quaternion conversion
    raise NotImplementedError(&#34;rotation_matrix_to_quaternion not yet implemented&#34;)</code></pre>
</details>
<div class="desc"><p>Convert 3x3 rotation matrices to quaternions.</p>
<p>Inverse of quaternion_to_rotation_matrix. Uses Shepperd's method
for numerical stability.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rot_matrices</code></strong></dt>
<dd>3x3 rotation matrices.
Shape: (N, 3, 3) or (3, 3)
dtype: float32
Should be orthogonal with det = +1</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>quats</code></dt>
<dd>Quaternions in (w, x, y, z) format.
Shape: (N, 4) or (4,)
dtype: float32
Normalized (||q|| = 1)</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; R = np.eye(3, dtype=np.float32)
&gt;&gt;&gt; q = rotation_matrix_to_quaternion(R)
&gt;&gt;&gt; print(q)  # [1, 0, 0, 0]
</code></pre></div>
</dd>
<dt id="gaussian_utils.sigmoid"><code class="name flex">
<span>def <span class="ident">sigmoid</span></span>(<span>x: Union[np.ndarray, Tensor]) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigmoid(x: Union[np.ndarray, Tensor]) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Sigmoid activation function.

    Used to convert logit-space opacity to [0, 1] range.

    Args:
        x: Input values in logit space.
            Shape: Any
            dtype: float32
            Range: (-inf, inf)

    Returns:
        Output values.
            Shape: Same as input
            Range: (0, 1)
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        return torch.sigmoid(x)
    return 1.0 / (1.0 + np.exp(-x))</code></pre>
</details>
<div class="desc"><p>Sigmoid activation function.</p>
<p>Used to convert logit-space opacity to [0, 1] range.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input values in logit space.
Shape: Any
dtype: float32
Range: (-inf, inf)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>Output values.</dt>
<dt><code>
Shape</code></dt>
<dd>Same as input
Range: (0, 1)</dd>
</dl></div>
</dd>
<dt id="gaussian_utils.spherical_harmonics_to_rgb"><code class="name flex">
<span>def <span class="ident">spherical_harmonics_to_rgb</span></span>(<span>sh_coeffs: Union[np.ndarray, Tensor],<br>view_directions: Optional[Union[np.ndarray, Tensor]] = None) ‑> numpy.ndarray | torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spherical_harmonics_to_rgb(
    sh_coeffs: Union[np.ndarray, Tensor],
    view_directions: Optional[Union[np.ndarray, Tensor]] = None,
) -&gt; Union[np.ndarray, Tensor]:
    &#34;&#34;&#34;
    Convert spherical harmonic coefficients back to RGB colors.

    Evaluates the spherical harmonic representation at given view directions
    to produce RGB colors. For degree-0 SH, the result is view-independent.

    Args:
        sh_coeffs: Spherical harmonic coefficients.
            Shape: (N, num_coeffs, 3)
            dtype: float32

        view_directions: Unit vectors pointing from surface to camera.
            Shape: (N, 3) or None
            dtype: float32
            Required if sh_coeffs has degree &gt; 0
            Each row should be normalized (||v|| = 1)
            Default: None (assumes degree 0, returns DC color)

    Returns:
        rgb: RGB color values.
            Shape: (N, 3)
            dtype: float32
            Range: Clipped to [0, 1]

    Example:
        &gt;&gt;&gt; sh = np.random.rand(100, 1, 3).astype(np.float32)
        &gt;&gt;&gt; rgb = spherical_harmonics_to_rgb(sh)
        &gt;&gt;&gt; print(rgb.shape)  # (100, 3)
    &#34;&#34;&#34;
    C0 = 0.28209479177387814

    is_tensor = isinstance(sh_coeffs, Tensor)

    # Get DC term (first coefficient)
    dc = sh_coeffs[:, 0, :]  # (N, 3)

    # Convert back to RGB: rgb = dc * C0 + 0.5
    rgb = dc * C0 + 0.5

    # Clamp to [0, 1]
    if is_tensor:
        rgb = torch.clamp(rgb, 0.0, 1.0)
    else:
        rgb = np.clip(rgb, 0.0, 1.0)

    return rgb</code></pre>
</details>
<div class="desc"><p>Convert spherical harmonic coefficients back to RGB colors.</p>
<p>Evaluates the spherical harmonic representation at given view directions
to produce RGB colors. For degree-0 SH, the result is view-independent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sh_coeffs</code></strong></dt>
<dd>Spherical harmonic coefficients.
Shape: (N, num_coeffs, 3)
dtype: float32</dd>
<dt><strong><code>view_directions</code></strong></dt>
<dd>Unit vectors pointing from surface to camera.
Shape: (N, 3) or None
dtype: float32
Required if sh_coeffs has degree &gt; 0
Each row should be normalized (||v|| = 1)
Default: None (assumes degree 0, returns DC color)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>rgb</code></dt>
<dd>RGB color values.
Shape: (N, 3)
dtype: float32
Range: Clipped to [0, 1]</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; sh = np.random.rand(100, 1, 3).astype(np.float32)
&gt;&gt;&gt; rgb = spherical_harmonics_to_rgb(sh)
&gt;&gt;&gt; print(rgb.shape)  # (100, 3)
</code></pre></div>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="gaussian_utils.build_covariance_3d" href="#gaussian_utils.build_covariance_3d">build_covariance_3d</a></code></li>
<li><code><a title="gaussian_utils.create_camera_intrinsics" href="#gaussian_utils.create_camera_intrinsics">create_camera_intrinsics</a></code></li>
<li><code><a title="gaussian_utils.create_front_camera_pose" href="#gaussian_utils.create_front_camera_pose">create_front_camera_pose</a></code></li>
<li><code><a title="gaussian_utils.create_orbit_camera_pose" href="#gaussian_utils.create_orbit_camera_pose">create_orbit_camera_pose</a></code></li>
<li><code><a title="gaussian_utils.depth_to_xyz" href="#gaussian_utils.depth_to_xyz">depth_to_xyz</a></code></li>
<li><code><a title="gaussian_utils.estimate_point_scales" href="#gaussian_utils.estimate_point_scales">estimate_point_scales</a></code></li>
<li><code><a title="gaussian_utils.get_view_direction" href="#gaussian_utils.get_view_direction">get_view_direction</a></code></li>
<li><code><a title="gaussian_utils.inverse_sigmoid" href="#gaussian_utils.inverse_sigmoid">inverse_sigmoid</a></code></li>
<li><code><a title="gaussian_utils.normalize_depth_to_metric" href="#gaussian_utils.normalize_depth_to_metric">normalize_depth_to_metric</a></code></li>
<li><code><a title="gaussian_utils.project_covariance_to_2d" href="#gaussian_utils.project_covariance_to_2d">project_covariance_to_2d</a></code></li>
<li><code><a title="gaussian_utils.quaternion_to_rotation_matrix" href="#gaussian_utils.quaternion_to_rotation_matrix">quaternion_to_rotation_matrix</a></code></li>
<li><code><a title="gaussian_utils.rgb_to_spherical_harmonics" href="#gaussian_utils.rgb_to_spherical_harmonics">rgb_to_spherical_harmonics</a></code></li>
<li><code><a title="gaussian_utils.rotation_matrix_to_quaternion" href="#gaussian_utils.rotation_matrix_to_quaternion">rotation_matrix_to_quaternion</a></code></li>
<li><code><a title="gaussian_utils.sigmoid" href="#gaussian_utils.sigmoid">sigmoid</a></code></li>
<li><code><a title="gaussian_utils.spherical_harmonics_to_rgb" href="#gaussian_utils.spherical_harmonics_to_rgb">spherical_harmonics_to_rgb</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
