<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>losses API documentation</title>
<meta name="description" content="Loss Functions for Gaussian Splatting Optimization …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>losses</code></h1>
</header>
<section id="section-intro">
<p>Loss Functions for Gaussian Splatting Optimization</p>
<p>This module provides loss functions used during Gaussian splatting training,
including photometric losses (L1, L2, SSIM), perceptual losses (LPIPS),
and regularization terms.</p>
<p>All losses are designed to work with masked regions, allowing optimization
only on the subject area while ignoring background.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="losses.compute_ssim_map"><code class="name flex">
<span>def <span class="ident">compute_ssim_map</span></span>(<span>img1: Tensor, img2: Tensor, window: Tensor, size_average: bool = True) ‑> torch.Tensor | Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_ssim_map(
    img1: Tensor,
    img2: Tensor,
    window: Tensor,
    size_average: bool = True,
) -&gt; Union[Tensor, Tuple[Tensor, Tensor]]:
    &#34;&#34;&#34;
    Compute SSIM map between two images.

    Args:
        img1: First image in (B, C, H, W) format.
        img2: Second image in (B, C, H, W) format.
        window: Gaussian window from gaussian_window().
        size_average: If True, return scalar mean. If False, return map.

    Returns:
        SSIM value (scalar) or SSIM map (H, W tensor).
    &#34;&#34;&#34;
    C1 = 0.01 ** 2
    C2 = 0.03 ** 2

    _, C, H, W = img1.shape

    # Expand window for all channels
    window = window.expand(C, 1, -1, -1)

    # Compute local means
    mu1 = F.conv2d(img1, window, padding=&#39;same&#39;, groups=C)
    mu2 = F.conv2d(img2, window, padding=&#39;same&#39;, groups=C)

    mu1_sq = mu1 ** 2
    mu2_sq = mu2 ** 2
    mu1_mu2 = mu1 * mu2

    # Compute local variances and covariance
    sigma1_sq = F.conv2d(img1 ** 2, window, padding=&#39;same&#39;, groups=C) - mu1_sq
    sigma2_sq = F.conv2d(img2 ** 2, window, padding=&#39;same&#39;, groups=C) - mu2_sq
    sigma12 = F.conv2d(img1 * img2, window, padding=&#39;same&#39;, groups=C) - mu1_mu2

    # SSIM formula
    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \
               ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

    if size_average:
        return ssim_map.mean()
    else:
        return ssim_map</code></pre>
</details>
<div class="desc"><p>Compute SSIM map between two images.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img1</code></strong></dt>
<dd>First image in (B, C, H, W) format.</dd>
<dt><strong><code>img2</code></strong></dt>
<dd>Second image in (B, C, H, W) format.</dd>
<dt><strong><code>window</code></strong></dt>
<dd>Gaussian window from gaussian_window().</dd>
<dt><strong><code>size_average</code></strong></dt>
<dd>If True, return scalar mean. If False, return map.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>SSIM value (scalar) or SSIM map (H, W tensor).</p></div>
</dd>
<dt id="losses.gaussian_window"><code class="name flex">
<span>def <span class="ident">gaussian_window</span></span>(<span>window_size: int, sigma: float, device: str = 'cpu') ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gaussian_window(window_size: int, sigma: float, device: str = &#34;cpu&#34;) -&gt; Tensor:
    &#34;&#34;&#34;
    Create a 2D Gaussian window for SSIM computation.

    Args:
        window_size: Size of the square window (should be odd).
        sigma: Standard deviation of the Gaussian.
        device: Computation device.

    Returns:
        Tensor: Normalized 2D Gaussian kernel.
            Shape: (1, 1, window_size, window_size)
            Sum equals 1.0
    &#34;&#34;&#34;
    # Create 1D Gaussian
    coords = torch.arange(window_size, dtype=torch.float32, device=device)
    coords = coords - window_size // 2

    g = torch.exp(-coords ** 2 / (2 * sigma ** 2))
    g = g / g.sum()

    # Create 2D Gaussian via outer product
    window_2d = g.unsqueeze(1) @ g.unsqueeze(0)  # (window_size, window_size)
    window_2d = window_2d / window_2d.sum()

    # Add batch and channel dimensions
    window_2d = window_2d.unsqueeze(0).unsqueeze(0)  # (1, 1, window_size, window_size)

    return window_2d</code></pre>
</details>
<div class="desc"><p>Create a 2D Gaussian window for SSIM computation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>window_size</code></strong></dt>
<dd>Size of the square window (should be odd).</dd>
<dt><strong><code>sigma</code></strong></dt>
<dd>Standard deviation of the Gaussian.</dd>
<dt><strong><code>device</code></strong></dt>
<dd>Computation device.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Normalized 2D Gaussian kernel.
Shape: (1, 1, window_size, window_size)
Sum equals 1.0</dd>
</dl></div>
</dd>
<dt id="losses.save_comparison_image"><code class="name flex">
<span>def <span class="ident">save_comparison_image</span></span>(<span>target: Tensor,<br>rendered: Tensor,<br>output_path: str,<br>mask: Optional[Tensor] = None,<br>title: str = 'Target | Rendered | Difference') ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_comparison_image(
    target: Tensor,
    rendered: Tensor,
    output_path: str,
    mask: Optional[Tensor] = None,
    title: str = &#34;Target | Rendered | Difference&#34;,
) -&gt; None:
    &#34;&#34;&#34;
    Save a side-by-side comparison image.

    Creates a visualization showing:
    - Left: Target/ground truth image
    - Center: Rendered image
    - Right: Absolute difference (enhanced for visibility)

    Args:
        target: Ground truth image.
            Shape: (H, W, 3)
            Range: [0, 1]

        rendered: Rendered image from Gaussian splatting.
            Shape: (H, W, 3)
            Range: [0, 1]

        output_path: Path to save the comparison image.

        mask: Optional binary mask to overlay.
            Shape: (H, W)

        title: Title for the image (not displayed, just for logging).
    &#34;&#34;&#34;
    import numpy as np
    import cv2

    # Convert to numpy
    target_np = target.detach().cpu().numpy()
    rendered_np = rendered.detach().cpu().numpy()

    # Clip to valid range
    target_np = np.clip(target_np, 0, 1)
    rendered_np = np.clip(rendered_np, 0, 1)

    # Compute difference
    diff_np = np.abs(target_np - rendered_np)

    # Enhance difference for visibility (scale by 5x)
    diff_enhanced = np.clip(diff_np * 5.0, 0, 1)

    # Concatenate horizontally
    comparison = np.concatenate([target_np, rendered_np, diff_enhanced], axis=1)

    # Add separator lines
    H, W, _ = target_np.shape
    comparison[:, W-1:W+1, :] = 1.0  # White line
    comparison[:, 2*W-1:2*W+1, :] = 1.0

    # Convert to BGR for OpenCV
    comparison_bgr = (comparison[:, :, ::-1] * 255).astype(np.uint8)

    # Save
    cv2.imwrite(output_path, comparison_bgr)
    print(f&#34;Saved comparison: {output_path}&#34;)</code></pre>
</details>
<div class="desc"><p>Save a side-by-side comparison image.</p>
<p>Creates a visualization showing:
- Left: Target/ground truth image
- Center: Rendered image
- Right: Absolute difference (enhanced for visibility)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>target</code></strong></dt>
<dd>Ground truth image.
Shape: (H, W, 3)
Range: [0, 1]</dd>
<dt><strong><code>rendered</code></strong></dt>
<dd>Rendered image from Gaussian splatting.
Shape: (H, W, 3)
Range: [0, 1]</dd>
<dt><strong><code>output_path</code></strong></dt>
<dd>Path to save the comparison image.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Optional binary mask to overlay.
Shape: (H, W)</dd>
<dt><strong><code>title</code></strong></dt>
<dd>Title for the image (not displayed, just for logging).</dd>
</dl></div>
</dd>
<dt id="losses.visualize_loss_components"><code class="name flex">
<span>def <span class="ident">visualize_loss_components</span></span>(<span>components: Dict[str, float], output_path: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize_loss_components(
    components: Dict[str, float],
    output_path: str,
) -&gt; None:
    &#34;&#34;&#34;
    Create a bar chart visualization of loss components.

    Args:
        components: Dictionary of loss component values.
        output_path: Path to save the chart.
    &#34;&#34;&#34;
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print(&#34;[WARN] matplotlib not installed, skipping loss visualization&#34;)
        return

    # Filter out zero values
    filtered = {k: v for k, v in components.items() if v &gt; 0 and k != &#39;total&#39;}

    if not filtered:
        return

    fig, ax = plt.subplots(figsize=(8, 4))

    names = list(filtered.keys())
    values = list(filtered.values())

    bars = ax.bar(names, values, color=[&#39;#2ecc71&#39;, &#39;#3498db&#39;, &#39;#9b59b6&#39;, &#39;#e74c3c&#39;, &#39;#f39c12&#39;])

    ax.set_ylabel(&#39;Loss Value&#39;)
    ax.set_title(f&#39;Loss Components (Total: {components.get(&#34;total&#34;, 0):.4f})&#39;)

    # Add value labels on bars
    for bar, val in zip(bars, values):
        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
                f&#39;{val:.4f}&#39;, ha=&#39;center&#39;, va=&#39;bottom&#39;, fontsize=9)

    plt.tight_layout()
    plt.savefig(output_path, dpi=100)
    plt.close()
    print(f&#34;Saved loss chart: {output_path}&#34;)</code></pre>
</details>
<div class="desc"><p>Create a bar chart visualization of loss components.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>components</code></strong></dt>
<dd>Dictionary of loss component values.</dd>
<dt><strong><code>output_path</code></strong></dt>
<dd>Path to save the chart.</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="losses.GaussianLosses"><code class="flex name class">
<span>class <span class="ident">GaussianLosses</span></span>
<span>(</span><span>weight_l1: float = 0.8,<br>weight_ssim: float = 0.2,<br>weight_lpips: float = 0.0,<br>weight_scale_reg: float = 0.0,<br>weight_opacity_reg: float = 0.0,<br>lpips_net: str = 'alex',<br>device: str = 'cpu')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GaussianLosses(nn.Module):
    &#34;&#34;&#34;
    Combined loss functions for Gaussian Splatting training.

    This class provides:
    - Photometric losses (L1, L2) for pixel-level reconstruction
    - Structural similarity (SSIM) for perceptual quality
    - LPIPS perceptual loss for high-level feature matching
    - Regularization losses for scale and opacity control

    Attributes:
        weight_l1: Weight for L1 photometric loss.
        weight_ssim: Weight for SSIM loss.
        weight_lpips: Weight for LPIPS perceptual loss.
        weight_scale_reg: Weight for scale regularization.
        weight_opacity_reg: Weight for opacity regularization.
        lpips_net: LPIPS network (VGG or Alex), loaded lazily.

    Example:
        &gt;&gt;&gt; losses = GaussianLosses(weight_l1=0.8, weight_ssim=0.2)
        &gt;&gt;&gt; rendered = model.render()  # (H, W, 3)
        &gt;&gt;&gt; target = ground_truth      # (H, W, 3)
        &gt;&gt;&gt; mask = segmentation_mask   # (H, W)
        &gt;&gt;&gt; total, components = losses.total_loss(rendered, target, mask)
        &gt;&gt;&gt; total.backward()
    &#34;&#34;&#34;

    def __init__(
        self,
        weight_l1: float = 0.8,
        weight_ssim: float = 0.2,
        weight_lpips: float = 0.0,
        weight_scale_reg: float = 0.0,
        weight_opacity_reg: float = 0.0,
        lpips_net: str = &#34;alex&#34;,
        device: str = &#34;cpu&#34;,
    ) -&gt; None:
        &#34;&#34;&#34;
        Initialize loss functions with specified weights.

        Args:
            weight_l1: Weight for L1 photometric loss.
                Default: 0.8
                Range: [0, 1] typical

            weight_ssim: Weight for SSIM structural loss.
                Default: 0.2
                Range: [0, 1] typical

            weight_lpips: Weight for LPIPS perceptual loss.
                Default: 0.0 (disabled)
                Range: [0, 0.5] typical
                Note: LPIPS is computationally expensive

            weight_scale_reg: Weight for scale regularization.
                Default: 0.0
                Penalizes very large or very small scales

            weight_opacity_reg: Weight for opacity regularization.
                Default: 0.0
                Encourages binary (0 or 1) opacities

            lpips_net: Network architecture for LPIPS.
                Options: &#34;vgg&#34;, &#34;alex&#34;
                Default: &#34;alex&#34; (faster than VGG)
                Only loaded if weight_lpips &gt; 0

            device: Computation device.
                Default: &#34;cpu&#34;
        &#34;&#34;&#34;
        super().__init__()
        self.weight_l1 = weight_l1
        self.weight_ssim = weight_ssim
        self.weight_lpips = weight_lpips
        self.weight_scale_reg = weight_scale_reg
        self.weight_opacity_reg = weight_opacity_reg
        self.device = device

        # Lazy load LPIPS if needed
        self._lpips_net = None
        self._lpips_net_type = lpips_net

        # Scale regularization thresholds (in log-space)
        self.scale_min_log = -7.0  # exp(-7) ≈ 0.001
        self.scale_max_log = 2.0   # exp(2) ≈ 7.4

    @property
    def lpips_net(self):
        &#34;&#34;&#34;Lazy-load LPIPS network on first use.&#34;&#34;&#34;
        if self._lpips_net is None and self.weight_lpips &gt; 0:
            try:
                import lpips
                self._lpips_net = lpips.LPIPS(net=self._lpips_net_type).to(self.device)
                self._lpips_net.eval()
                for param in self._lpips_net.parameters():
                    param.requires_grad = False
            except ImportError:
                print(&#34;[WARN] lpips not installed, LPIPS loss will return 0&#34;)
                self._lpips_net = None
        return self._lpips_net

    def photometric_loss(
        self,
        rendered: Tensor,
        target: Tensor,
        mask: Optional[Tensor] = None,
        loss_type: str = &#34;l1&#34;,
    ) -&gt; Tensor:
        &#34;&#34;&#34;
        Compute pixel-wise photometric loss between rendered and target images.

        Args:
            rendered: Rendered RGB image from Gaussian splatting.
                Shape: (H, W, 3) or (B, H, W, 3)
                dtype: float32
                Range: [0, 1]

            target: Ground truth RGB image.
                Shape: (H, W, 3) or (B, H, W, 3)
                dtype: float32
                Range: [0, 1]

            mask: Optional binary mask for valid regions.
                Shape: (H, W) or (B, H, W)
                dtype: float32 or bool
                Values: 1 = valid, 0 = ignore
                If None, all pixels are used.
                Default: None

            loss_type: Type of photometric loss.
                Options: &#34;l1&#34;, &#34;l2&#34;, &#34;huber&#34;
                Default: &#34;l1&#34;

        Returns:
            Tensor: Scalar loss value.
        &#34;&#34;&#34;
        # Ensure same shape
        if rendered.shape != target.shape:
            raise ValueError(f&#34;Shape mismatch: rendered {rendered.shape} vs target {target.shape}&#34;)

        # Compute pixel-wise difference
        if loss_type == &#34;l1&#34;:
            diff = torch.abs(rendered - target)
        elif loss_type == &#34;l2&#34;:
            diff = (rendered - target) ** 2
        elif loss_type == &#34;huber&#34;:
            diff = F.smooth_l1_loss(rendered, target, reduction=&#39;none&#39;)
        else:
            raise ValueError(f&#34;Unknown loss type: {loss_type}&#34;)

        # Apply mask if provided
        if mask is not None:
            # Expand mask to match image dimensions
            if mask.ndim == 2:
                mask = mask.unsqueeze(-1)  # (H, W, 1)
            elif mask.ndim == 3 and mask.shape[-1] != 1:
                mask = mask.unsqueeze(-1)

            mask = mask.float()

            # Masked mean
            diff = diff * mask
            num_valid = mask.sum() + 1e-8
            loss = diff.sum() / num_valid
        else:
            loss = diff.mean()

        return loss

    def ssim_loss(
        self,
        img1: Tensor,
        img2: Tensor,
        mask: Optional[Tensor] = None,
        window_size: int = 11,
    ) -&gt; Tensor:
        &#34;&#34;&#34;
        Compute Structural Similarity Index (SSIM) loss.

        SSIM measures perceptual similarity considering luminance,
        contrast, and structure. The loss is 1 - SSIM, so lower is better.

        Args:
            img1: First image (typically rendered).
                Shape: (H, W, 3) or (B, H, W, 3)
                dtype: float32
                Range: [0, 1]

            img2: Second image (typically target).
                Shape: (H, W, 3) or (B, H, W, 3)
                dtype: float32
                Range: [0, 1]

            mask: Optional binary mask.
                Shape: (H, W) or (B, H, W)
                If None, all pixels are used.
                Default: None

            window_size: Size of Gaussian window for local statistics.
                Default: 11
                Should be odd number

        Returns:
            Tensor: Scalar loss value in range [0, 1].
                0 = identical images
                1 = completely different
        &#34;&#34;&#34;
        # Use pytorch_msssim for efficient SSIM computation
        try:
            from pytorch_msssim import ssim as compute_ssim
        except ImportError:
            # Fallback to simple implementation
            return self._ssim_simple(img1, img2, mask, window_size)

        # Convert from (H, W, 3) to (B, C, H, W) format
        if img1.ndim == 3:
            img1 = img1.permute(2, 0, 1).unsqueeze(0)  # (1, 3, H, W)
            img2 = img2.permute(2, 0, 1).unsqueeze(0)

        # Compute SSIM
        ssim_val = compute_ssim(
            img1, img2,
            data_range=1.0,
            size_average=True,
            win_size=window_size,
        )

        # Return 1 - SSIM as loss
        loss = 1.0 - ssim_val

        return loss

    def _ssim_simple(
        self,
        img1: Tensor,
        img2: Tensor,
        mask: Optional[Tensor] = None,
        window_size: int = 11,
    ) -&gt; Tensor:
        &#34;&#34;&#34;Simple SSIM implementation as fallback.&#34;&#34;&#34;
        C1 = 0.01 ** 2
        C2 = 0.03 ** 2

        # Create Gaussian window
        window = gaussian_window(window_size, 1.5, device=img1.device)

        # Convert to (B, C, H, W) format
        if img1.ndim == 3:
            img1 = img1.permute(2, 0, 1).unsqueeze(0)
            img2 = img2.permute(2, 0, 1).unsqueeze(0)

        _, C, H, W = img1.shape

        # Pad images
        pad = window_size // 2
        img1 = F.pad(img1, (pad, pad, pad, pad), mode=&#39;reflect&#39;)
        img2 = F.pad(img2, (pad, pad, pad, pad), mode=&#39;reflect&#39;)

        # Expand window for all channels
        window = window.expand(C, 1, window_size, window_size)

        # Compute local means
        mu1 = F.conv2d(img1, window, groups=C)
        mu2 = F.conv2d(img2, window, groups=C)

        mu1_sq = mu1 ** 2
        mu2_sq = mu2 ** 2
        mu1_mu2 = mu1 * mu2

        # Compute local variances and covariance
        sigma1_sq = F.conv2d(img1 ** 2, window, groups=C) - mu1_sq
        sigma2_sq = F.conv2d(img2 ** 2, window, groups=C) - mu2_sq
        sigma12 = F.conv2d(img1 * img2, window, groups=C) - mu1_mu2

        # SSIM formula
        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \
                   ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

        # Mean SSIM
        ssim_val = ssim_map.mean()

        return 1.0 - ssim_val

    def lpips_loss(
        self,
        rendered: Tensor,
        target: Tensor,
        mask: Optional[Tensor] = None,
    ) -&gt; Tensor:
        &#34;&#34;&#34;
        Compute LPIPS (Learned Perceptual Image Patch Similarity) loss.

        LPIPS uses deep features from a pretrained network to measure
        perceptual similarity, often correlating better with human
        perception than pixel-wise metrics.

        Args:
            rendered: Rendered RGB image.
                Shape: (H, W, 3) or (B, H, W, 3)
                dtype: float32
                Range: [0, 1]

            target: Ground truth RGB image.
                Shape: (H, W, 3) or (B, H, W, 3)
                dtype: float32
                Range: [0, 1]

            mask: Optional binary mask (applied after LPIPS computation).
                Shape: (H, W) or (B, H, W)
                Note: LPIPS is computed on full images, mask only affects
                the final averaging.
                Default: None

        Returns:
            Tensor: Scalar loss value.
                Range: [0, ~1], lower = more similar
                Typical good reconstruction: &lt; 0.1
        &#34;&#34;&#34;
        if self.lpips_net is None:
            return torch.tensor(0.0, device=rendered.device)

        # Convert from (H, W, 3) to (B, C, H, W) format
        if rendered.ndim == 3:
            rendered = rendered.permute(2, 0, 1).unsqueeze(0)  # (1, 3, H, W)
            target = target.permute(2, 0, 1).unsqueeze(0)

        # LPIPS expects range [-1, 1]
        rendered_lpips = rendered * 2.0 - 1.0
        target_lpips = target * 2.0 - 1.0

        # Compute LPIPS
        with torch.no_grad():
            # Set to eval mode during LPIPS computation
            self._lpips_net.eval()

        lpips_val = self._lpips_net(rendered_lpips, target_lpips)

        return lpips_val.mean()

    def regularization_loss(
        self,
        scales: Tensor,
        opacities: Tensor,
    ) -&gt; Tuple[Tensor, Tensor]:
        &#34;&#34;&#34;
        Compute regularization losses for Gaussian parameters.

        Regularization helps prevent degenerate solutions and improves
        training stability. Two types are provided:

        1. Scale regularization: Prevents Gaussians from becoming too large
           or too small, which can cause rendering artifacts.

        2. Opacity regularization: Encourages binary opacities (0 or 1),
           reducing the number of semi-transparent Gaussians.

        Args:
            scales: Log-scale parameters for all Gaussians.
                Shape: (N, 3)
                dtype: float32
                Range: Log-space (exp(scales) gives actual scale)

            opacities: Logit-opacity parameters for all Gaussians.
                Shape: (N,) or (N, 1)
                dtype: float32
                Range: Logit-space (sigmoid(opacities) gives actual opacity)

        Returns:
            Tuple[Tensor, Tensor]:
                - scale_loss: Scalar regularization loss for scales
                - opacity_loss: Scalar regularization loss for opacities
        &#34;&#34;&#34;
        # ===== Scale Regularization =====
        # Penalize scales that are too large or too small
        # scales is in log-space

        # Penalize scales below minimum (too small -&gt; numerical issues)
        scale_min_penalty = F.relu(self.scale_min_log - scales)

        # Penalize scales above maximum (too large -&gt; rendering artifacts)
        scale_max_penalty = F.relu(scales - self.scale_max_log)

        scale_loss = (scale_min_penalty ** 2).mean() + (scale_max_penalty ** 2).mean()

        # ===== Opacity Regularization =====
        # Encourage binary opacities (either 0 or 1)
        # This helps with pruning and reduces floaters

        # Convert from logit to actual opacity
        if opacities.ndim == 2:
            opacities = opacities.squeeze(-1)

        opacity_sigmoid = torch.sigmoid(opacities)

        # Binary entropy: p * (1 - p) is minimized when p = 0 or p = 1
        opacity_loss = (opacity_sigmoid * (1.0 - opacity_sigmoid)).mean()

        return scale_loss, opacity_loss

    def total_loss(
        self,
        rendered: Tensor,
        target: Tensor,
        mask: Optional[Tensor] = None,
        scales: Optional[Tensor] = None,
        opacities: Optional[Tensor] = None,
    ) -&gt; Tuple[Tensor, Dict[str, float]]:
        &#34;&#34;&#34;
        Compute total weighted loss combining all components.

        This is the main entry point for loss computation during training.
        It combines photometric (L1), structural (SSIM), perceptual (LPIPS),
        and regularization losses with configurable weights.

        Args:
            rendered: Rendered RGB image from Gaussian splatting.
                Shape: (H, W, 3)
                dtype: float32
                Range: [0, 1]

            target: Ground truth RGB image.
                Shape: (H, W, 3)
                dtype: float32
                Range: [0, 1]

            mask: Binary mask for valid regions.
                Shape: (H, W)
                dtype: float32 or bool
                If None, all pixels are used.
                Default: None

            scales: Log-scale parameters (for regularization).
                Shape: (N, 3)
                If None, scale regularization is skipped.
                Default: None

            opacities: Logit-opacity parameters (for regularization).
                Shape: (N,)
                If None, opacity regularization is skipped.
                Default: None

        Returns:
            Tuple[Tensor, Dict[str, float]]:
                - total: Scalar total loss (differentiable)
                - components: Dictionary of individual loss values (for logging)
                    Keys: &#34;l1&#34;, &#34;ssim&#34;, &#34;lpips&#34;, &#34;scale_reg&#34;, &#34;opacity_reg&#34;, &#34;total&#34;
        &#34;&#34;&#34;
        components = {}
        total = torch.tensor(0.0, device=rendered.device, requires_grad=True)

        # L1 loss
        if self.weight_l1 &gt; 0:
            l1_loss = self.photometric_loss(rendered, target, mask, loss_type=&#34;l1&#34;)
            components[&#34;l1&#34;] = l1_loss.item()
            total = total + self.weight_l1 * l1_loss

        # SSIM loss
        if self.weight_ssim &gt; 0:
            ssim_loss = self.ssim_loss(rendered, target, mask)
            components[&#34;ssim&#34;] = ssim_loss.item()
            total = total + self.weight_ssim * ssim_loss

        # LPIPS loss
        if self.weight_lpips &gt; 0:
            lpips_loss = self.lpips_loss(rendered, target, mask)
            components[&#34;lpips&#34;] = lpips_loss.item()
            total = total + self.weight_lpips * lpips_loss
        else:
            components[&#34;lpips&#34;] = 0.0

        # Regularization losses
        if scales is not None and opacities is not None:
            scale_reg, opacity_reg = self.regularization_loss(scales, opacities)

            if self.weight_scale_reg &gt; 0:
                components[&#34;scale_reg&#34;] = scale_reg.item()
                total = total + self.weight_scale_reg * scale_reg
            else:
                components[&#34;scale_reg&#34;] = 0.0

            if self.weight_opacity_reg &gt; 0:
                components[&#34;opacity_reg&#34;] = opacity_reg.item()
                total = total + self.weight_opacity_reg * opacity_reg
            else:
                components[&#34;opacity_reg&#34;] = 0.0
        else:
            components[&#34;scale_reg&#34;] = 0.0
            components[&#34;opacity_reg&#34;] = 0.0

        components[&#34;total&#34;] = total.item()

        return total, components</code></pre>
</details>
<div class="desc"><p>Combined loss functions for Gaussian Splatting training.</p>
<p>This class provides:
- Photometric losses (L1, L2) for pixel-level reconstruction
- Structural similarity (SSIM) for perceptual quality
- LPIPS perceptual loss for high-level feature matching
- Regularization losses for scale and opacity control</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>weight_l1</code></strong></dt>
<dd>Weight for L1 photometric loss.</dd>
<dt><strong><code>weight_ssim</code></strong></dt>
<dd>Weight for SSIM loss.</dd>
<dt><strong><code>weight_lpips</code></strong></dt>
<dd>Weight for LPIPS perceptual loss.</dd>
<dt><strong><code>weight_scale_reg</code></strong></dt>
<dd>Weight for scale regularization.</dd>
<dt><strong><code>weight_opacity_reg</code></strong></dt>
<dd>Weight for opacity regularization.</dd>
<dt><strong><code>lpips_net</code></strong></dt>
<dd>LPIPS network (VGG or Alex), loaded lazily.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; losses = GaussianLosses(weight_l1=0.8, weight_ssim=0.2)
&gt;&gt;&gt; rendered = model.render()  # (H, W, 3)
&gt;&gt;&gt; target = ground_truth      # (H, W, 3)
&gt;&gt;&gt; mask = segmentation_mask   # (H, W)
&gt;&gt;&gt; total, components = losses.total_loss(rendered, target, mask)
&gt;&gt;&gt; total.backward()
</code></pre>
<p>Initialize loss functions with specified weights.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>weight_l1</code></strong></dt>
<dd>Weight for L1 photometric loss.
Default: 0.8
Range: [0, 1] typical</dd>
<dt><strong><code>weight_ssim</code></strong></dt>
<dd>Weight for SSIM structural loss.
Default: 0.2
Range: [0, 1] typical</dd>
<dt><strong><code>weight_lpips</code></strong></dt>
<dd>Weight for LPIPS perceptual loss.
Default: 0.0 (disabled)
Range: [0, 0.5] typical
Note: LPIPS is computationally expensive</dd>
<dt><strong><code>weight_scale_reg</code></strong></dt>
<dd>Weight for scale regularization.
Default: 0.0
Penalizes very large or very small scales</dd>
<dt><strong><code>weight_opacity_reg</code></strong></dt>
<dd>Weight for opacity regularization.
Default: 0.0
Encourages binary (0 or 1) opacities</dd>
<dt><strong><code>lpips_net</code></strong></dt>
<dd>Network architecture for LPIPS.
Options: "vgg", "alex"
Default: "alex" (faster than VGG)
Only loaded if weight_lpips &gt; 0</dd>
<dt><strong><code>device</code></strong></dt>
<dd>Computation device.
Default: "cpu"</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="losses.GaussianLosses.lpips_net"><code class="name">prop <span class="ident">lpips_net</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def lpips_net(self):
    &#34;&#34;&#34;Lazy-load LPIPS network on first use.&#34;&#34;&#34;
    if self._lpips_net is None and self.weight_lpips &gt; 0:
        try:
            import lpips
            self._lpips_net = lpips.LPIPS(net=self._lpips_net_type).to(self.device)
            self._lpips_net.eval()
            for param in self._lpips_net.parameters():
                param.requires_grad = False
        except ImportError:
            print(&#34;[WARN] lpips not installed, LPIPS loss will return 0&#34;)
            self._lpips_net = None
    return self._lpips_net</code></pre>
</details>
<div class="desc"><p>Lazy-load LPIPS network on first use.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="losses.GaussianLosses.lpips_loss"><code class="name flex">
<span>def <span class="ident">lpips_loss</span></span>(<span>self, rendered: Tensor, target: Tensor, mask: Optional[Tensor] = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lpips_loss(
    self,
    rendered: Tensor,
    target: Tensor,
    mask: Optional[Tensor] = None,
) -&gt; Tensor:
    &#34;&#34;&#34;
    Compute LPIPS (Learned Perceptual Image Patch Similarity) loss.

    LPIPS uses deep features from a pretrained network to measure
    perceptual similarity, often correlating better with human
    perception than pixel-wise metrics.

    Args:
        rendered: Rendered RGB image.
            Shape: (H, W, 3) or (B, H, W, 3)
            dtype: float32
            Range: [0, 1]

        target: Ground truth RGB image.
            Shape: (H, W, 3) or (B, H, W, 3)
            dtype: float32
            Range: [0, 1]

        mask: Optional binary mask (applied after LPIPS computation).
            Shape: (H, W) or (B, H, W)
            Note: LPIPS is computed on full images, mask only affects
            the final averaging.
            Default: None

    Returns:
        Tensor: Scalar loss value.
            Range: [0, ~1], lower = more similar
            Typical good reconstruction: &lt; 0.1
    &#34;&#34;&#34;
    if self.lpips_net is None:
        return torch.tensor(0.0, device=rendered.device)

    # Convert from (H, W, 3) to (B, C, H, W) format
    if rendered.ndim == 3:
        rendered = rendered.permute(2, 0, 1).unsqueeze(0)  # (1, 3, H, W)
        target = target.permute(2, 0, 1).unsqueeze(0)

    # LPIPS expects range [-1, 1]
    rendered_lpips = rendered * 2.0 - 1.0
    target_lpips = target * 2.0 - 1.0

    # Compute LPIPS
    with torch.no_grad():
        # Set to eval mode during LPIPS computation
        self._lpips_net.eval()

    lpips_val = self._lpips_net(rendered_lpips, target_lpips)

    return lpips_val.mean()</code></pre>
</details>
<div class="desc"><p>Compute LPIPS (Learned Perceptual Image Patch Similarity) loss.</p>
<p>LPIPS uses deep features from a pretrained network to measure
perceptual similarity, often correlating better with human
perception than pixel-wise metrics.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rendered</code></strong></dt>
<dd>Rendered RGB image.
Shape: (H, W, 3) or (B, H, W, 3)
dtype: float32
Range: [0, 1]</dd>
<dt><strong><code>target</code></strong></dt>
<dd>Ground truth RGB image.
Shape: (H, W, 3) or (B, H, W, 3)
dtype: float32
Range: [0, 1]</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Optional binary mask (applied after LPIPS computation).
Shape: (H, W) or (B, H, W)
Note: LPIPS is computed on full images, mask only affects
the final averaging.
Default: None</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Scalar loss value.
Range: [0, ~1], lower = more similar
Typical good reconstruction: &lt; 0.1</dd>
</dl></div>
</dd>
<dt id="losses.GaussianLosses.photometric_loss"><code class="name flex">
<span>def <span class="ident">photometric_loss</span></span>(<span>self,<br>rendered: Tensor,<br>target: Tensor,<br>mask: Optional[Tensor] = None,<br>loss_type: str = 'l1') ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def photometric_loss(
    self,
    rendered: Tensor,
    target: Tensor,
    mask: Optional[Tensor] = None,
    loss_type: str = &#34;l1&#34;,
) -&gt; Tensor:
    &#34;&#34;&#34;
    Compute pixel-wise photometric loss between rendered and target images.

    Args:
        rendered: Rendered RGB image from Gaussian splatting.
            Shape: (H, W, 3) or (B, H, W, 3)
            dtype: float32
            Range: [0, 1]

        target: Ground truth RGB image.
            Shape: (H, W, 3) or (B, H, W, 3)
            dtype: float32
            Range: [0, 1]

        mask: Optional binary mask for valid regions.
            Shape: (H, W) or (B, H, W)
            dtype: float32 or bool
            Values: 1 = valid, 0 = ignore
            If None, all pixels are used.
            Default: None

        loss_type: Type of photometric loss.
            Options: &#34;l1&#34;, &#34;l2&#34;, &#34;huber&#34;
            Default: &#34;l1&#34;

    Returns:
        Tensor: Scalar loss value.
    &#34;&#34;&#34;
    # Ensure same shape
    if rendered.shape != target.shape:
        raise ValueError(f&#34;Shape mismatch: rendered {rendered.shape} vs target {target.shape}&#34;)

    # Compute pixel-wise difference
    if loss_type == &#34;l1&#34;:
        diff = torch.abs(rendered - target)
    elif loss_type == &#34;l2&#34;:
        diff = (rendered - target) ** 2
    elif loss_type == &#34;huber&#34;:
        diff = F.smooth_l1_loss(rendered, target, reduction=&#39;none&#39;)
    else:
        raise ValueError(f&#34;Unknown loss type: {loss_type}&#34;)

    # Apply mask if provided
    if mask is not None:
        # Expand mask to match image dimensions
        if mask.ndim == 2:
            mask = mask.unsqueeze(-1)  # (H, W, 1)
        elif mask.ndim == 3 and mask.shape[-1] != 1:
            mask = mask.unsqueeze(-1)

        mask = mask.float()

        # Masked mean
        diff = diff * mask
        num_valid = mask.sum() + 1e-8
        loss = diff.sum() / num_valid
    else:
        loss = diff.mean()

    return loss</code></pre>
</details>
<div class="desc"><p>Compute pixel-wise photometric loss between rendered and target images.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rendered</code></strong></dt>
<dd>Rendered RGB image from Gaussian splatting.
Shape: (H, W, 3) or (B, H, W, 3)
dtype: float32
Range: [0, 1]</dd>
<dt><strong><code>target</code></strong></dt>
<dd>Ground truth RGB image.
Shape: (H, W, 3) or (B, H, W, 3)
dtype: float32
Range: [0, 1]</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Optional binary mask for valid regions.
Shape: (H, W) or (B, H, W)
dtype: float32 or bool
Values: 1 = valid, 0 = ignore
If None, all pixels are used.
Default: None</dd>
<dt><strong><code>loss_type</code></strong></dt>
<dd>Type of photometric loss.
Options: "l1", "l2", "huber"
Default: "l1"</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Scalar loss value.</dd>
</dl></div>
</dd>
<dt id="losses.GaussianLosses.regularization_loss"><code class="name flex">
<span>def <span class="ident">regularization_loss</span></span>(<span>self, scales: Tensor, opacities: Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def regularization_loss(
    self,
    scales: Tensor,
    opacities: Tensor,
) -&gt; Tuple[Tensor, Tensor]:
    &#34;&#34;&#34;
    Compute regularization losses for Gaussian parameters.

    Regularization helps prevent degenerate solutions and improves
    training stability. Two types are provided:

    1. Scale regularization: Prevents Gaussians from becoming too large
       or too small, which can cause rendering artifacts.

    2. Opacity regularization: Encourages binary opacities (0 or 1),
       reducing the number of semi-transparent Gaussians.

    Args:
        scales: Log-scale parameters for all Gaussians.
            Shape: (N, 3)
            dtype: float32
            Range: Log-space (exp(scales) gives actual scale)

        opacities: Logit-opacity parameters for all Gaussians.
            Shape: (N,) or (N, 1)
            dtype: float32
            Range: Logit-space (sigmoid(opacities) gives actual opacity)

    Returns:
        Tuple[Tensor, Tensor]:
            - scale_loss: Scalar regularization loss for scales
            - opacity_loss: Scalar regularization loss for opacities
    &#34;&#34;&#34;
    # ===== Scale Regularization =====
    # Penalize scales that are too large or too small
    # scales is in log-space

    # Penalize scales below minimum (too small -&gt; numerical issues)
    scale_min_penalty = F.relu(self.scale_min_log - scales)

    # Penalize scales above maximum (too large -&gt; rendering artifacts)
    scale_max_penalty = F.relu(scales - self.scale_max_log)

    scale_loss = (scale_min_penalty ** 2).mean() + (scale_max_penalty ** 2).mean()

    # ===== Opacity Regularization =====
    # Encourage binary opacities (either 0 or 1)
    # This helps with pruning and reduces floaters

    # Convert from logit to actual opacity
    if opacities.ndim == 2:
        opacities = opacities.squeeze(-1)

    opacity_sigmoid = torch.sigmoid(opacities)

    # Binary entropy: p * (1 - p) is minimized when p = 0 or p = 1
    opacity_loss = (opacity_sigmoid * (1.0 - opacity_sigmoid)).mean()

    return scale_loss, opacity_loss</code></pre>
</details>
<div class="desc"><p>Compute regularization losses for Gaussian parameters.</p>
<p>Regularization helps prevent degenerate solutions and improves
training stability. Two types are provided:</p>
<ol>
<li>
<p>Scale regularization: Prevents Gaussians from becoming too large
or too small, which can cause rendering artifacts.</p>
</li>
<li>
<p>Opacity regularization: Encourages binary opacities (0 or 1),
reducing the number of semi-transparent Gaussians.</p>
</li>
</ol>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scales</code></strong></dt>
<dd>Log-scale parameters for all Gaussians.
Shape: (N, 3)
dtype: float32
Range: Log-space (exp(scales) gives actual scale)</dd>
<dt><strong><code>opacities</code></strong></dt>
<dd>Logit-opacity parameters for all Gaussians.
Shape: (N,) or (N, 1)
dtype: float32
Range: Logit-space (sigmoid(opacities) gives actual opacity)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tuple[Tensor, Tensor]:
- scale_loss: Scalar regularization loss for scales
- opacity_loss: Scalar regularization loss for opacities</p></div>
</dd>
<dt id="losses.GaussianLosses.ssim_loss"><code class="name flex">
<span>def <span class="ident">ssim_loss</span></span>(<span>self,<br>img1: Tensor,<br>img2: Tensor,<br>mask: Optional[Tensor] = None,<br>window_size: int = 11) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ssim_loss(
    self,
    img1: Tensor,
    img2: Tensor,
    mask: Optional[Tensor] = None,
    window_size: int = 11,
) -&gt; Tensor:
    &#34;&#34;&#34;
    Compute Structural Similarity Index (SSIM) loss.

    SSIM measures perceptual similarity considering luminance,
    contrast, and structure. The loss is 1 - SSIM, so lower is better.

    Args:
        img1: First image (typically rendered).
            Shape: (H, W, 3) or (B, H, W, 3)
            dtype: float32
            Range: [0, 1]

        img2: Second image (typically target).
            Shape: (H, W, 3) or (B, H, W, 3)
            dtype: float32
            Range: [0, 1]

        mask: Optional binary mask.
            Shape: (H, W) or (B, H, W)
            If None, all pixels are used.
            Default: None

        window_size: Size of Gaussian window for local statistics.
            Default: 11
            Should be odd number

    Returns:
        Tensor: Scalar loss value in range [0, 1].
            0 = identical images
            1 = completely different
    &#34;&#34;&#34;
    # Use pytorch_msssim for efficient SSIM computation
    try:
        from pytorch_msssim import ssim as compute_ssim
    except ImportError:
        # Fallback to simple implementation
        return self._ssim_simple(img1, img2, mask, window_size)

    # Convert from (H, W, 3) to (B, C, H, W) format
    if img1.ndim == 3:
        img1 = img1.permute(2, 0, 1).unsqueeze(0)  # (1, 3, H, W)
        img2 = img2.permute(2, 0, 1).unsqueeze(0)

    # Compute SSIM
    ssim_val = compute_ssim(
        img1, img2,
        data_range=1.0,
        size_average=True,
        win_size=window_size,
    )

    # Return 1 - SSIM as loss
    loss = 1.0 - ssim_val

    return loss</code></pre>
</details>
<div class="desc"><p>Compute Structural Similarity Index (SSIM) loss.</p>
<p>SSIM measures perceptual similarity considering luminance,
contrast, and structure. The loss is 1 - SSIM, so lower is better.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img1</code></strong></dt>
<dd>First image (typically rendered).
Shape: (H, W, 3) or (B, H, W, 3)
dtype: float32
Range: [0, 1]</dd>
<dt><strong><code>img2</code></strong></dt>
<dd>Second image (typically target).
Shape: (H, W, 3) or (B, H, W, 3)
dtype: float32
Range: [0, 1]</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Optional binary mask.
Shape: (H, W) or (B, H, W)
If None, all pixels are used.
Default: None</dd>
<dt><strong><code>window_size</code></strong></dt>
<dd>Size of Gaussian window for local statistics.
Default: 11
Should be odd number</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Scalar loss value in range [0, 1].
0 = identical images
1 = completely different</dd>
</dl></div>
</dd>
<dt id="losses.GaussianLosses.total_loss"><code class="name flex">
<span>def <span class="ident">total_loss</span></span>(<span>self,<br>rendered: Tensor,<br>target: Tensor,<br>mask: Optional[Tensor] = None,<br>scales: Optional[Tensor] = None,<br>opacities: Optional[Tensor] = None) ‑> Tuple[torch.Tensor, Dict[str, float]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def total_loss(
    self,
    rendered: Tensor,
    target: Tensor,
    mask: Optional[Tensor] = None,
    scales: Optional[Tensor] = None,
    opacities: Optional[Tensor] = None,
) -&gt; Tuple[Tensor, Dict[str, float]]:
    &#34;&#34;&#34;
    Compute total weighted loss combining all components.

    This is the main entry point for loss computation during training.
    It combines photometric (L1), structural (SSIM), perceptual (LPIPS),
    and regularization losses with configurable weights.

    Args:
        rendered: Rendered RGB image from Gaussian splatting.
            Shape: (H, W, 3)
            dtype: float32
            Range: [0, 1]

        target: Ground truth RGB image.
            Shape: (H, W, 3)
            dtype: float32
            Range: [0, 1]

        mask: Binary mask for valid regions.
            Shape: (H, W)
            dtype: float32 or bool
            If None, all pixels are used.
            Default: None

        scales: Log-scale parameters (for regularization).
            Shape: (N, 3)
            If None, scale regularization is skipped.
            Default: None

        opacities: Logit-opacity parameters (for regularization).
            Shape: (N,)
            If None, opacity regularization is skipped.
            Default: None

    Returns:
        Tuple[Tensor, Dict[str, float]]:
            - total: Scalar total loss (differentiable)
            - components: Dictionary of individual loss values (for logging)
                Keys: &#34;l1&#34;, &#34;ssim&#34;, &#34;lpips&#34;, &#34;scale_reg&#34;, &#34;opacity_reg&#34;, &#34;total&#34;
    &#34;&#34;&#34;
    components = {}
    total = torch.tensor(0.0, device=rendered.device, requires_grad=True)

    # L1 loss
    if self.weight_l1 &gt; 0:
        l1_loss = self.photometric_loss(rendered, target, mask, loss_type=&#34;l1&#34;)
        components[&#34;l1&#34;] = l1_loss.item()
        total = total + self.weight_l1 * l1_loss

    # SSIM loss
    if self.weight_ssim &gt; 0:
        ssim_loss = self.ssim_loss(rendered, target, mask)
        components[&#34;ssim&#34;] = ssim_loss.item()
        total = total + self.weight_ssim * ssim_loss

    # LPIPS loss
    if self.weight_lpips &gt; 0:
        lpips_loss = self.lpips_loss(rendered, target, mask)
        components[&#34;lpips&#34;] = lpips_loss.item()
        total = total + self.weight_lpips * lpips_loss
    else:
        components[&#34;lpips&#34;] = 0.0

    # Regularization losses
    if scales is not None and opacities is not None:
        scale_reg, opacity_reg = self.regularization_loss(scales, opacities)

        if self.weight_scale_reg &gt; 0:
            components[&#34;scale_reg&#34;] = scale_reg.item()
            total = total + self.weight_scale_reg * scale_reg
        else:
            components[&#34;scale_reg&#34;] = 0.0

        if self.weight_opacity_reg &gt; 0:
            components[&#34;opacity_reg&#34;] = opacity_reg.item()
            total = total + self.weight_opacity_reg * opacity_reg
        else:
            components[&#34;opacity_reg&#34;] = 0.0
    else:
        components[&#34;scale_reg&#34;] = 0.0
        components[&#34;opacity_reg&#34;] = 0.0

    components[&#34;total&#34;] = total.item()

    return total, components</code></pre>
</details>
<div class="desc"><p>Compute total weighted loss combining all components.</p>
<p>This is the main entry point for loss computation during training.
It combines photometric (L1), structural (SSIM), perceptual (LPIPS),
and regularization losses with configurable weights.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>rendered</code></strong></dt>
<dd>Rendered RGB image from Gaussian splatting.
Shape: (H, W, 3)
dtype: float32
Range: [0, 1]</dd>
<dt><strong><code>target</code></strong></dt>
<dd>Ground truth RGB image.
Shape: (H, W, 3)
dtype: float32
Range: [0, 1]</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Binary mask for valid regions.
Shape: (H, W)
dtype: float32 or bool
If None, all pixels are used.
Default: None</dd>
<dt><strong><code>scales</code></strong></dt>
<dd>Log-scale parameters (for regularization).
Shape: (N, 3)
If None, scale regularization is skipped.
Default: None</dd>
<dt><strong><code>opacities</code></strong></dt>
<dd>Logit-opacity parameters (for regularization).
Shape: (N,)
If None, opacity regularization is skipped.
Default: None</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>Tuple[Tensor, Dict[str, float]]:</dt>
<dt>- total: Scalar total loss (differentiable)</dt>
<dt>- components: Dictionary of individual loss values (for logging)</dt>
<dt><code>
Keys</code></dt>
<dd>"l1", "ssim", "lpips", "scale_reg", "opacity_reg", "total"</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="losses.compute_ssim_map" href="#losses.compute_ssim_map">compute_ssim_map</a></code></li>
<li><code><a title="losses.gaussian_window" href="#losses.gaussian_window">gaussian_window</a></code></li>
<li><code><a title="losses.save_comparison_image" href="#losses.save_comparison_image">save_comparison_image</a></code></li>
<li><code><a title="losses.visualize_loss_components" href="#losses.visualize_loss_components">visualize_loss_components</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="losses.GaussianLosses" href="#losses.GaussianLosses">GaussianLosses</a></code></h4>
<ul class="two-column">
<li><code><a title="losses.GaussianLosses.lpips_loss" href="#losses.GaussianLosses.lpips_loss">lpips_loss</a></code></li>
<li><code><a title="losses.GaussianLosses.lpips_net" href="#losses.GaussianLosses.lpips_net">lpips_net</a></code></li>
<li><code><a title="losses.GaussianLosses.photometric_loss" href="#losses.GaussianLosses.photometric_loss">photometric_loss</a></code></li>
<li><code><a title="losses.GaussianLosses.regularization_loss" href="#losses.GaussianLosses.regularization_loss">regularization_loss</a></code></li>
<li><code><a title="losses.GaussianLosses.ssim_loss" href="#losses.GaussianLosses.ssim_loss">ssim_loss</a></code></li>
<li><code><a title="losses.GaussianLosses.total_loss" href="#losses.GaussianLosses.total_loss">total_loss</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
