{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 01 \u2014 Single Person Demo\n\nThis notebook runs the Human3D pipeline on a **single-person** image and visualizes the main outputs:\n\n- Depth map (`depth.npy`, `depth.png`)\n- Pose overlay (`pose_overlay.png`)\n- Segmentation mask and overlay (`seg_mask.png`, `seg_overlay.png`)\n- Point cloud (`pointcloud.ply`)\n\nIt is meant to be readable and reproducible, not fancy."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# --- Standard imports ---\nimport os\nfrom pathlib import Path\nimport json\nimport numpy as np\n\n# --- Visualization ---\nimport matplotlib.pyplot as plt\n\n# Make plots bigger by default (no fixed colors here)\nplt.rcParams[\"figure.figsize\"] = (10, 6)\n\n# --- Repo root detection (works when notebook is in ./notebooks) ---\nREPO_ROOT = Path.cwd()\nif (REPO_ROOT / \"src\").exists() and (REPO_ROOT / \"configs\").exists():\n    pass\nelif (REPO_ROOT.parent / \"src\").exists() and (REPO_ROOT.parent / \"configs\").exists():\n    REPO_ROOT = REPO_ROOT.parent\nelse:\n    raise RuntimeError(\"Could not locate repo root. Open this notebook from inside the repo folder.\")\n\nprint(\"Repo root:\", REPO_ROOT)\n\n# Ensure src is importable\nimport sys\nSRC_DIR = REPO_ROOT / \"src\"\nif str(SRC_DIR) not in sys.path:\n    sys.path.insert(0, str(SRC_DIR))\n\n# Imports from the project\nfrom human3d.utils.config import load_config\nfrom human3d.pipeline import Human3DPipeline"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Choose an input image\n\nPut an image in `data/` (or `data/sample_images/`) and set `INPUT_PATH` below.\n\n**Tip:** For the best single-person result, use a full-body photo with the subject clearly visible and minimal occlusion."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Path to your single-person image (edit this)\nINPUT_PATH = REPO_ROOT / \"data\" / \"sample.jpg\"\n\nassert INPUT_PATH.exists(), f\"Input image not found: {INPUT_PATH}\"\nprint(\"Using input:\", INPUT_PATH)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Load config and run the pipeline"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "CFG_PATH = REPO_ROOT / \"configs\" / \"pipeline.yaml\"\ncfg = load_config(str(CFG_PATH))\n\npipe = Human3DPipeline(cfg)\nout_dir = Path(pipe.run(str(INPUT_PATH)))\n\nprint(\"Outputs saved to:\", out_dir)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Inspect outputs\n\nThis section loads the output images and shows them inline."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from PIL import Image\n\ndef show_img(p: Path, title: str):\n    if not p.exists():\n        print(f\"[MISS] {p.name} not found\")\n        return\n    img = Image.open(p)\n    plt.figure()\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n\nshow_img(out_dir / \"depth.png\", \"Depth visualization (MiDaS)\")\nshow_img(out_dir / \"pose_overlay.png\", \"Pose overlay (YOLOv8-Pose)\")\nshow_img(out_dir / \"seg_mask.png\", \"Segmentation mask (SAM)\")\nshow_img(out_dir / \"seg_overlay.png\", \"Segmentation overlay\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Load numeric depth and basic stats"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "depth_path = out_dir / \"depth.npy\"\ndepth = np.load(depth_path)\n\nprint(\"depth shape:\", depth.shape)\nprint(\"depth dtype:\", depth.dtype)\nprint(\"depth min/max:\", float(depth.min()), float(depth.max()))\nprint(\"depth nonzero ratio:\", float((depth > 0).mean()))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Point cloud preview (optional)\n\nPoint clouds can be viewed with:\n\n- **Open3D** (interactive window)\n- **CloudCompare / MeshLab** (open `pointcloud.ply`)\n\nIf Open3D fails to open a window (common on some systems), just open the `.ply` file externally."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "pc_path = out_dir / \"pointcloud.ply\"\nprint(\"PLY:\", pc_path)\n\ntry:\n    import open3d as o3d\n    pcd = o3d.io.read_point_cloud(str(pc_path))\n    print(pcd)\n    # Interactive viewer (may not work in some notebook environments)\n    o3d.visualization.draw_geometries([pcd])\nexcept Exception as e:\n    print(\"Open3D preview failed:\", repr(e))\n    print(\"You can open the PLY file in CloudCompare or MeshLab instead.\")"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "title": "01_single_person_demo"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}